---
title: "Implementation Multiple Time Series Analysis"
author: "Jacopo Lussetti"
date: "2025-04-02"
header-includes:
  - \usepackage{amsmath}
  - \DeclareMathOperator{\VEC}{vec}
  - \usepackage{bm}  # Bold math symbols
output:
  pdf_document: default 
  html_document: default
bibliography: reference.bib    

---
```{r, echo=FALSE, warning=FALSE, message=FALSE}
#install required packages
packages <- c("MASS", "zoo", "stats", "rockchalk", "glmnet", "vars", "knitr", "car", "caret", "psych", "dplyr")

for (pckg in packages) {
  if (!require(pckg, character.only = TRUE)) {
    install.packages(pckg)
    library(pckg, character.only = TRUE)
  } else {
    library(pckg, character.only = TRUE)
  }
}
```   

# VAR(p) functions with VAR(1) representation


## Companion Matrix function 


```{r}
comp_mtrx <- function(AA){
    ## AA is a K x Kp matrix, so we are able to derive p in the following way
    K <- nrow(AA)
    Kp <- ncol(AA)      
    p <- Kp/K
    
    # Create the empty companion matrix Kp x Kp
    C <- matrix(0, nrow=Kp, ncol=Kp)

    C[1:K,] <- AA
    
    # Add ones on the K-th sub-diagonal
    if (p>1)
        C[(K+1):Kp, 1:(Kp-K)] <- diag(Kp - K)
    return(C)
}
```

## Autocovariance function

Equation (2.1.39) page 29 represents the formula to compute autocovariances of 
a \textit{stable} VAR(p) Process. First and foremost we will then have to evaluate
whether the process is stable 

### Stability check 

A VAR(p) is a \textbf{stable process} if the condition (2.1.9.) holds: 

$$
det(I- \mathbf{A}z) \neq 0 \ if \ |z|<1 \ \text{where z are eigenvalues for A}
$$

```{r}
  var_roots<-function(AA){
    if(nrow(AA)==ncol(AA)){  # matrix is squared
      C<-AA  
    }else{
      C<-comp_mtrx(AA) # transform form compact matrix into companion matrix 
    }
    eig<-eigen(C)$values
    return(eig) 
  }
```

### Formula
After evaluating the conditions for stability, we proceed then defining the 
formula to compute contrivances. Th


\begin{equation}
  \VEC \ \Gamma_Y(0)= (I_{(Kp)^2}- \mathbf{A} \otimes \mathbf{A})^{-1} \VEC \ \Sigma_U
\end{equation}


```{r}
autocov_fun<-function(A, Sigma_u,p=1){ # A for high-order var is combined matrix,
  K<-nrow(Sigma_u) 
  Kp<-K * p
  #for var(1) is just A1
 if(p>1){
   #compute companion
   A<- comp_mtrx(A)
   #extend original sigma_u
  Sigma_U<-matrix(0, nrow=Kp, ncol=Kp) 
  Sigma_U[1:K, 1:K]<-Sigma_u
   
 }else{
   Sigma_U<-Sigma_u
 } 
 # compute the Kronecker product
  I<-diag(1, Kp^2)
  #compute vectorised Sigma_U
  vec_Sigma_U<-as.vector(Sigma_U)
  # compute the Autocovariance function
  vec_gamma_0<-solve(I - kronecker(A, A)) %*% vec_Sigma_U
  # reshape the result into a matrix
  Gamma_Y_0<-matrix(vec_gamma_0, nrow=Kp, ncol=Kp)
  
  return(Gamma_Y_0)  
}
```
## Equilibrium Points

Equilibrium points are defined by formula 2.1.10 at page 16

\begin{equation}
  \mathbf{\mu}:= E(Y_t)= (I_{Kp}-\mathbf{A})^{-1} \mathbf{\nu}
\end{equation}

```{r}
equilibrium<-function(A, nu){
  #check stability condition
  eig<-var_roots(A)
  if(any(Mod(eig)>=1)){   
      stop("Trajectories are not stable")
  }
  Kp<-nrow(A)
  I_Kp<-diag(1,Kp)
  values<-solve(I_Kp-A) %*% nu
  return(values)
}
```



## VAR(p) model

For this case we just consider the function simulating the trajectories, whereas
equilibrium and autocovariance functions are treated separately. 

```{r}
  var_sim <- function(AA, nu, Sigma_u, nSteps, y0) {
    K <- nrow(Sigma_u)
    Kp <- ncol(AA)
    p <- Kp/K
        
    if (p > 1) {
        C <- comp_mtrx(AA) # form the  companion matrix of the var(p) process
    } else {
        C <- AA  
    }
    y_t <- matrix(0, nrow =  nSteps, ncol=Kp) #trajectories matrix nSteps x Kp
    y_t[1, 1:Kp] <- y0 #add initial value to initiate the simulation
    noise <- mvrnorm(n = nSteps, mu = rep(0, K), Sigma = Sigma_u) #assuming that 
    #residuals follow a multivariate normal distribution    
    
    for (t in 2:nSteps) {
        y_t[t, ] <- C %*% y_t[t-1, ]
        y_t[t, 1:K] <- y_t[t, 1:K] + nu + noise[t,]
    }
    
    y_t <- zoo(y_t[,1:K], 1:nSteps)  
    return(y_t)
  }
```



# Estimates

## Estimates of coefficents
We first define a formula to compute Z values, which are key parameters for 
both estimating coefficient \& auto-correlation matrix. 

```{r}
par_estimate <- function(y_t, p=1) {
  nObs <- nrow(y_t)  # Number of observations
  K <- ncol(y_t)  # Number of variables
  T <- nObs - p  # Number of usable observations

  # y 
  Y <- y_t[(p + 1):nObs, ]  # T x K matrix

  # Z
  Z <- matrix(1, nrow = T, ncol = (K * p + 1))  # Intercept + lagged values

  for (i in 1:p) {
    col_start <- 2 + (i - 1) * K
    col_end <- 1 + i * K
    Z[, col_start:col_end] <- y_t[(p + 1 - i):(nObs - i), ]
  }

  # Estimate coefficients using OLS: B_hat = (Z'Z)^(-1) Z'Y
  B_hat_t<- solve(t(Z) %*% Z) %*% t(Z) %*% Y  # (K*p + 1) x K matrix
  B_hat<-t(B_hat_t)
  return(list(
    Y = Y,
    Z = Z,
    B_hat = B_hat,  # Estimated VAR parameters
    T = T
  ))
}
```

```{r}
  estimator<-function(Y, Z, p=1, method = c("standard", "qr", "lsfit")) {
  
      # Estimator
       method <- match.arg(method)
       if(method == "standard"){
          K <- ncol(Y)
          Kp<-K*p
          I_k<-diag(1, ncol(Y))
          vec_y<-as.vector(Y)
          B_hat <- solve(t(Z) %*% Z, t(Z) %*% Y)
          B_hat <- t(B_hat)
          
       } else if(method == "qr"){
          qr_decomp <- qr(Z)  
          B_hat <- qr.coef(qr_decomp, Y)
       } else if(method == "lsfit"){
         fit <- lsfit(Z, Y) 
          B_hat <- fit$coef  
       } else {
          stop("Unknown method")
       }
      return(list(nu=B_hat[,1], AA=B_hat[,-1]))
    }
```
  

## Estimates of the autocovariance function

```{r}
  est_autocov <- function(y_t, Y, Z, T, p=1){
    K <- ncol(y_t)
    Kp <- K * p
    I_t <- diag(T)

    # QR decomposition of Z to avoid singularity issues
    qr_decomp <- qr(Z)
    Q <- qr.Q(qr_decomp)
    P_Z <- Q %*% t(Q)  # Projection matrix

    # Compute bias-corrected covariance
    bias_sigma <- 1/T * t(Y) %*% (I_t - P_Z) %*% Y

    # Degrees of freedom correction
    d.f. <- T / (T - Kp - 1)
    unbiased <- d.f. * bias_sigma  # Corrected covariance estimate

    return(unbiased)
}

```

# Simulation \& Estimation
  
## Test A


```{r, fig.width=7, fig.height=5}
set.seed(123)
  p<-1
  A<-matrix(c(0.5, 0.1, 0., 0., 0.1, 0.2, 0., 0.3, 0.3), nrow=3, ncol=3)
  nu<-matrix(c(0.05, 0.02, 0.04), nrow=3)
  Sigma_u <- matrix(c(1.0, 0.2, 0.3, 0.2, 2.0, 0.5, 0.3, 0.5, 3.0), nrow = 3, ncol = 3)
  nSteps <- 10000
  y0 <- matrix(c(0.1, -0.1, 0.2),ncol=ncol(A))

#compute trajectories
y_t_a<-var_sim(A, nu, Sigma_u, nSteps, y0)  
plot(y_t_a[1:1000,], main = "VAR(1) Simulation", xlab = "Time", ylab = "y values", col = 1:3, lty = 1)
```
### Multivariate Least Squares Estimators


```{r}
#estimate parameters
par_A<-par_estimate(y_t_a)
#estimate coefficient 

test_A<-estimator(par_A$Y, par_A$Z)
auto_cov_A<-est_autocov(y_t_a, par_A$Y, par_A$Z, par_A$T)

```
Now we will compare original input for the simulation \& estimated values

Coefficient Matrix
```{r}
A_true <- A  
A_est <- test_A$AA 

diff_A <- A_true - A_est  
print(diff_A)

```
Intercept Matrix
```{r}
nu_true <- nu  # True nu from the simulation
nu_est <- test_A$nu  # Estimated nu

diff_nu <- nu_true - nu_est  # Compute the difference
print(diff_nu)
```
Covariance Matrix

```{r}
Sigma_u_true <- Sigma_u  # True covariance matrix
Sigma_u_est <- auto_cov_A  # Estimated autocovariance

diff_Sigma_u <- Sigma_u_true - Sigma_u_est  # Compute the difference
print(diff_Sigma_u)

```

### Variance Sensitivity Analysis

```{r}
set.seed(123)
A_intrc<-cbind(nu, A)
sigma_shrinked<-Sigma_u*0.5
sigma_scale<-Sigma_u*3
y_t_a_shrinked<-var_sim(A, nu, sigma_shrinked, nSteps, y0)
y_t_a_scaled<-var_sim(A, nu, sigma_scale, nSteps, y0)
#compute the estimates
y_t_a_shrinked_par<-par_estimate(y_t_a_shrinked)$B_hat
y_t_a_scaled_par<-par_estimate(y_t_a_scaled)$B_hat
#compare with original 
A_true_A<-par_estimate(y_t_a)$B_hat
#mse function
mse <- function(A_true, A_est) {
    A_diff <- A_true - A_est
    mse <- sqrt(sum(A_diff^2)) / (nrow(A_true) * ncol(A_true))
    return(mse)
}
mse_shrinked<-mse(A_intrc, y_t_a_shrinked_par)
mse_scaled<-mse(A_intrc, y_t_a_scaled_par)
mse_normal<-mse(A_intrc, A_true_A)
summary<-data.frame(
  transformation=c("Shrinked", "Normal", "Scaled"),
  MSE=c(mse_shrinked, mse_normal, mse_scaled)
)
summary

```

From statistical theory we know that OLS estimator is asymptotically normal with variance equal to the reverse of the Fisher Information Matrix. We know that for 
VAR process, the Fisher information for the coefficient matrix $B$ is given by
\[
\mathcal{I}_B = \Sigma_u^{-1} \otimes (Z^{\top}Z),
\]
and its inverse, representing the asymptotic covariance of the OLS estimator, is
\[
\operatorname{Var}\!\left(\mathrm{vec}(\hat{B})\right)
= \Sigma_u \otimes (Z^{\top}Z)^{-1}.
\]
This implies that by scaling the covariance matrix of the residuals $\Sigma_u$, we are
scaling the variance of the OLS estimator proportionally. Therefore, increasing
the variance of the residuals will lead to a higher mean squared error (MSE) in the estimated coefficients, while decreasing the variance will result in a lower MSE. This relationship highlights the sensitivity of the OLS estimator to changes in the residual variance.

## Test B

Kp-dimensional representation for VAR(p) is defined as following:

\begin{align*}
Y_t &= \nu + \mathbf{A} Y_{t-1} + U_t \\
\text{where} \quad
Y_t &:= 
\begin{bmatrix}
y_t \\
y_{t-1} \\
\vdots \\
y_{t-p+1}
\end{bmatrix}, \quad
\nu :=
\begin{bmatrix}
\nu \\
0 \\
\text{($K p \times 1$ zeros)}
\end{bmatrix}, \\
\mathbf{A} &:=
\begin{bmatrix}
A_1 & A_2 & \ldots & A_{p-1} & A_p \\
I_K & 0 & \ldots & 0 & 0 \\
0 & I_K & \ldots & 0 & 0 \\
\vdots & & \ddots & \vdots & \vdots \\
0 & 0 & \ldots & I_K & 0
\end{bmatrix}, \quad
U_t :=
\begin{bmatrix}
u_t \\
0 \\
\vdots \\
0
\end{bmatrix}
\end{align*}

We first start with the simulation and we also include a variance-sensitivity analysis

```{r, fig.width=7, fig.height=5}
# Define parameters for Test B

set.seed(1234567)
p_B <- 2  # Number of lags for VAR(p)
nSteps_B <- 10000
K_B <- 2  # Number of variables (size of y_t)
Sigma_u_B <- matrix(c(2, 0.3, 0.3, 3), nrow = 2, ncol = 2)
y0_B <- c(0.5, 2, 1, 5) 
nu_int_B <- matrix(c(0.5, 0.9), nrow = 2)

A_1 <- matrix(c(0.5, 0.4, 0.1, 0.5), nrow = 2, ncol = 2)
A_2 <- matrix(c(0.15, 0.10, 0.05, 0.20), nrow = 2, ncol = 2)
AA <- cbind(A_1, A_2)
AA_fin<-cbind(nu_int_B, AA)
# Simulate time series for Test B
y_t_B <- var_sim(AA, nu_int_B, Sigma_u_B, nSteps_B, y0_B)
A_est_B<-par_estimate(y_t_B, p = p_B)$B_hat
#compute different sigmas
Sigma_u_shrinked<- 0.5* Sigma_u_B
# Simulate time series for Test B
y_t_B_shrinkage <- var_sim(AA, nu_int_B, Sigma_u_shrinked, nSteps_B, y0_B)
#now we compare with the previous plot
Sigma_u_expanded<- 3* Sigma_u_B
y_t_B_expanded <- var_sim(AA, nu_int_B, Sigma_u_expanded, nSteps_B, y0_B)

matplot(y_t_B,  type = "l", main = "VAR(2) Simulation",
        xlab = "Time", ylab = "y values", col = 1:2, lty = 1)
matplot(y_t_B_shrinkage, type = "l", main = "VAR(2) Simulation (Shrinkage)",
        xlab = "Time", ylab = "y values", col = 1:2, lty = 1)

matplot(y_t_B_expanded, type = "l", main = "VAR(2) Simulation (Expansion)",
        xlab = "Time", ylab = "y values", col = 1:2, lty = 1)
```


```{r}
  
  # Estimate the parameters for Shrinkage
  par_B_shrk <- par_estimate(y_t_B_shrinkage, p = p_B)
  A_hat_shrk <- par_B_shrk$B_hat
  
  #we repeat for the Expansion
  par_B_exp <- par_estimate(y_t_B_expanded, p = p_B)
  A_hat_exp<- par_B_exp$B_hat # remove intercepts
  AA_fin
  A_hat_shrk
  A_hat_exp
  #we compute MSE
  
  #compute relative true coefficient matrix
  mse_shrk<- mse(AA_fin, A_hat_shrk)
  mse_exp<- mse(AA_fin, A_hat_exp)
  mse_norm<- mse(AA_fin, A_est_B)
  summary<-data.frame(
    transformation=c(0.5,1, 3),
    MSE=c(mse_shrk, mse_norm, mse_exp)
  )
  summary
```

Results confirm what previously mentioned in the Test A. This confirms that this hold for var process with higher order p. 

```{r}
# Estimate the parameters for Test B
par_B <- par_estimate(y_t_B, p = p_B)

# Estimate coefficients
A_est_B <- par_B$B_hat # A including the intercepts
#intercept
test_B <- estimator(par_B$Y, par_B$Z)
nu_est_B <- test_B$nu
# Estimate autocovariance matrix for Test B
auto_cov_B <- est_autocov(y_t_B, par_B$Y, par_B$Z, par_B$T, p = p_B)

# Display the estimated autocovariance matrix
auto_cov_B

```
Coefficient Matrix

```{r}
# True and estimated coefficient matrix for Test B

diff_A_B <- AA_fin - A_est_B  # Difference between true and estimated A

print("Difference in coefficient matrix (A):")
print(diff_A_B)

```
Intercept Matrix

```{r}
# True and estimated intercept matrix for Test B
nu_true_B <- nu_int_B  # True intercept vector
diff_nu_B <- nu_true_B - nu_est_B  # Difference between true and estimated nu

print("Difference in intercept matrix (nu):")
print(diff_nu_B)

```
Covariance Matrix

```{r}
# True and estimated covariance matrix for Test B
Sigma_u_true_B <- Sigma_u_B  # True covariance matrix
diff_Sigma_u_B <- Sigma_u_true_B - auto_cov_B  # Difference between true and estimated covariance

print("Difference in covariance matrix (Sigma_u):")
print(diff_Sigma_u_B)


```

# VAR(1) simulation with sparese coefficient Matrix

\begin{enumerate}
  \item \textbf{VAR(1) Simulation:} \\
  Generate data from the process
  \[
    y_t = A y_{t-1} + u_t, \quad u_t \sim \mathcal{N}(0, \sigma^2 I).
  \]
  where A is a lower triangular matrix with sparsity on the top right. 
  \item \textbf{Estimation via LASSO:} \\
  Apply LASSO regression to estimate the autoregressive matrix $A$ from the simulated data through R funtion \textit{glmnet}.
  \item \textbf{Optimisation of the tuning parameter} \\
    Through cross-validation or information criterion, and we compare strategies
    \item  \textbf{Monte Carlo Repetition}
  \item \textbf{Evaluation of Errors:} \\
  compute Type I and Type II error rates in detecting zero vs.~nonzero coefficients.
  \item \textbf{Increase dimension}\\
  We try larger matrices by increasing K, and we observe how LASSO’s performance changes as dimension increases.
\end{enumerate}

## VAR(1) Simulation

The first step is to simulate the true coefficient matrix such that the VAR(1) process
generated will be stable. To do so, we will generate a strictly lower triangular matrix
such that all eigenvalues are equal to zero. This is ensured by the foundamental theorem of trace: \\
\begin{equation*}
\text{trace}(A)= \Sigma_{i=1}^n a_{ii}= \Sigma_{i=1}^n \lambda_i
\end{equation*}

```{r}
set.seed(1234)
stab_test <- function(kp, A, tol = 1e-8)
{
  if (!is.matrix(A) || nrow(A) != ncol(A)) {
    stop("The matrix is not square")
  }
  eig <- eigen(A, only.values = TRUE)$values  # computing the eigenvalues
  
  for (i in 1:length(eig)) {     
    if (Mod(eig[i]) >= 1 - tol) { # Mod also handles complex numbers
      return(FALSE)               # <-- fixed typo "returm"
    }
  }
  return(TRUE)
}

#In this case there is no need to check on the stability, as according to 
#the foundamental theorem of trace, the sum of the eigenvalues of a matrix is equal to
#the sum of its diagonal elements. Since we are generating an upper triangular matrix
#with all diagonal elements equal to zero, the sum of the eigenvalues will also be zero.

A_sim <- function(K, spar, sd, max_tries = 1000){ #paramete spar determine
  #how many elements will be set to zero
  tries <- 0 
  repeat{
    A <- matrix(0L, K, K)
    idx_up <- which(upper.tri(A))
    n_up   <- length(idx_up)

    if (n_up > 0) {
      A[idx_up] <- rnorm(n_up, mean = 0, sd = sd)   #fill the upper 
  #triangle with r.v. from a normal distribution
  #cancel a certain percentage of the elements
      nmiss <- round(n_up * spar)
      if (nmiss > 0) { # add sparsity
        zero_idx <- sample(idx_up, nmiss, replace = FALSE)
        A[zero_idx] <- 0
      }
    }

    # we apply the formula previously defined to check stability
    if (stab_test(K, A) == TRUE) return(A)
     # else try again
    tries <- tries + 1
    #to prevent infinite loop due to using repeat loop, we set a max number of
    #iterations
    if (tries >= max_tries) {
      stop("Could not generate a stable A within max_tries.")
    }
  }
}

#now we generate a stable upper triangle coef matrix with sparsity 
K <- 5 
sd_var<-0.1
A <- A_sim(K, spar = 0.1, sd = sd_var, max_tries = 10) # moderate sparsity, modest sd A
stab_test(K,A)
```
Now we are able to generate a VAR(1) from the function specified at page 3

```{r}

sigma_var<- diag(1,5,5)* sd_var
nu_var <- rep(0, K)  
y_0 <- rep(0, K)  
T_sim<-5000
var_1<- var_sim(AA=A,nu=nu_var,Sigma_u=sigma_var, nSteps=T_sim, y0=y_0)
plot(var_1[0:1000,], main = "VAR(1) Simulation",type="l", xlab = "Time", ylab = "y values", col = 1:5, lty = 1)
```


## Estimation via LASSO

In this step we will apply lasso regression by applying the packages \textit{glmnet}


```{r}
# Build lagged design (Z) and aligned response (Y)
Z <- as.matrix(var_1[- nrow(var_1),])  # predictors y_{t-1}
# Drop first row to align (remove the NA created by lag)
Y <- as.matrix(var_1[-1, , drop = FALSE])
# 70/30 split 
n<-nrow(Z)
cut_off <- round(0.70 * n)
X_train <- Z[1:cut_off, , drop = FALSE]
Y_train <- Y[1:cut_off, , drop = FALSE]
X_test <- Z[(cut_off + 1):n, , drop = FALSE]
Y_test <- Y[(cut_off + 1):n, , drop = FALSE]
# Manual CV over a simple lambda grid (avoid 0 exactly)
lambdas <- seq(0,1, 0.0002)
val_error <- data.frame(lambda = lambdas, error = NA_real_)
for (i in seq_along(lambdas)) {
lmbd <- lambdas[i]
fit <- glmnet(X_train, Y_train,intercept = FALSE, family = "mgaussian", lambda = lmbd)
pred <- predict(fit, newx = X_test, s = lmbd)[,,1]
mse_per_series <- colMeans((Y_test - pred)^2)
val_error$error[i] <- mean(mse_per_series)
}
best_lambda_1 <- val_error$lambda[which.min(val_error$error)]
best_lambda_1
```
```{r}
#analyse coefficients
fit<-glmnet(
  x=as.matrix(Z),
  y = as.matrix(Y),
  intercept=FALSE,
  family = "mgaussian",
  lambda =best_lambda_1
)
coef_list <- coef(fit, s = best_lambda_1)

#export coef into a unique matrix 
A_hat<-matrix(0L, 5,5)
for(j in 1:5){
  coef<-as.matrix(coef_list[[j]])
  A_hat[j,]<-coef[!rownames(coef)=="(Intercept)",1] #intercepts are 0 anyway
}
A_hat
A
#check theorem 1 whether the euclidean norm of the ture non zero and the estimated one
# is of the same order as Op(n^{-1/2} log(p)^{1/2})  
tol<-min(abs(A[A!=0]))/4
A_hat>tol

```
From this trial we notice that for large sparse matrices, therefore with lower 
number of non zero coeffiecients, classical hypertuning methods such as cross-validation might be likely to compute lamba values that are too small. This 
lead to a smaller discriminations, and therefore higher type I error.

```{r}
cv_fit <- cv.glmnet(
  x = as.matrix(Z),
  y = as.matrix(Y),
  intercept=FALSE, # our process has nu=0, therefore we remove the intercept
  family = "mgaussian"  # var us a mutlivariate regression
)

plot(cv_fit)
best_lambda_2 <- cv_fit$lambda.1se # lambda.min brings results that are 
#far not optimal, and have higher type I error
best_lambda_2
best_lambda_2_bis<-cv_fit$lambda.min
best_lambda_2_bis
```

Discrimination of lambda based on the mellow $C_p$ criteria, BIC and AIC defined 
as follows:

\begin{table}[!h]
\centering
\caption{Summary of model selection criteria based on degrees of freedom}
\label{tab:criteria-df}
\begin{tabular}{ll}
\hline
\textbf{Criterion} & \textbf{Formula} \\
\hline
$C_p$   & $\displaystyle \|y - \hat{\mu}\|^{2} + 2\,\tau^{2}\,df$ \\
AIC     & $\displaystyle N\log(2\pi\tau^{2}) + \frac{\|y - \hat{\mu}\|^{2}}{\tau^{2}} + 2\,df$ \\
BIC     & $\displaystyle N\log(2\pi\tau^{2}) + \frac{\|y - \hat{\mu}\|^{2}}{\tau^{2}} + (\log N)\,df$ \\
\hline
\end{tabular}
\end{table}
When we have normal residuals and no sparsity, the d.f can be computed as follows:
\begin{align*}
d.f. &= \Sigma_{i=1}^N \frac{\text{cov}(\hat{\mu}_i, y_i)}{\sigma^2}\\
     & \text{tr}(X(X^TX)^{-1}X^T) = \\
     & \text{rank}(X)
\end{align*}
```{r}
criteria_df<-function(X,Y, p,steps){
n<-nrow(X)
#recall that the min lambda for gaussian is C'sigma sqrt(log(p)/n)
C_i <- apply(X, 2, function(col) sqrt(sum(col^2)) / sqrt(n))
C <- max(C_i)
#we compute an initial estimator of sigma from the OLS
beta_ols <- solve(t(X) %*% X) %*% t(X) %*% Y
resid <- Y - X %*% beta_ols
sigma_hat <- sqrt(sum(resid^2) / (n - p))
lambda_0<- C * sigma_hat * sqrt(log(p) / n) # initial lambda
#create a seq of candidate lambda with increments = steps
lambda_seq<-seq(lambda_0, 10*lambda_0, by=steps)
len_lambda<-length(lambda_seq)
#create a data frame to store the results
results<-data.frame(
lambda_seq=lambda_seq,
df=as.numeric(rep(0, len_lambda)),
Cp=as.numeric(rep(0, len_lambda)),
AIC=as.numeric(rep(0, len_lambda)),
BIC=as.numeric(rep(0, len_lambda))
)
#we compute now the compute estimate by using glmnet
for(i in 1:length(lambda_seq)){
fit<-glmnet(
X,
Y,
intercept = FALSE,
family = "mgaussian",
lambda = lambda_seq[i]
)
#extract df
df<-fit$df
results$df[i]<-df
#compute sd
Y_hat<-predict(fit, newx=X)[,,1]
RSS <- sum((Y - Y_hat)^2)
sigma_hat<-RSS /(n-df)
#compute criteria
results$Cp[i]<-RSS + 2*sigma_hat*df
results$AIC[i]<-n*log(2*pi*sigma_hat) + RSS/sigma_hat + 2*df
results$BIC[i]<-n*log(2*pi*sigma_hat) + RSS/sigma_hat + (log(n))*df
}
return(results)
}
test<-criteria_df(Z, Y, p=5, steps=0.0001)
index<-which.min(test$Cp)
print(test[index,])
lambda_criterion<-test$lambda[index]
```

```{r}
#function to extract the coefficient 
A_hat <- function(coef_list, K) {
  Ahat <- matrix(NA_real_, K, K)
  for (j in 1:K) {
    cj <- as.matrix(coef_list[[j]])
    Ahat[j, ] <- cj[rownames(cj) != "(Intercept)", 1]  # drop intercept
  }
  Ahat
}
#oracle property: tol level 
metrics <- function(A_true, A_hat, tol = 5e-3) { # where tol ~ 1/4 of the minimum non zero coefficient
  S_true <- abs(A_true) > 0
  S_est  <- abs(A_hat)  > tol

  TP <- sum(S_true & S_est)
  FN <- sum(S_true & !S_est)
  FP <- sum(!S_true & S_est)
  TN <- sum(!S_true & !S_est)

  FPR <- if ((FP + TN) > 0) FP / (FP + TN) else NA_real_
  FNR <- if ((FN + TP) > 0) FN / (FN + TP) else NA_real_
  Prec <- if ((TP + FP) > 0) TP / (TP + FP) else NA_real_
  Rec  <- if ((TP + FN) > 0) TP / (TP + FN) else NA_real_
  F1   <- if (!is.na(Prec + Rec) && (Prec + Rec) > 0) 2 * Prec * Rec / (Prec + Rec) else NA_real_

  # coefficient MSE (Frobenius): mean squared diff over all K^2 entries
  MSE <- mean((A_true - A_hat)^2)

  c(TypeI = FPR, TypeII = FNR, Recall = Rec, Precision = Prec, F1 = F1, MSE = MSE)
}

#fit the model with the tuning parameters
lambda_cand<-c(best_lambda_1,best_lambda_2_bis,best_lambda_2, lambda_criterion, 0.03)

fit_opt<-glmnet(
  x<-as.matrix(Z),
  y<-as.matrix(Y),
  intercept=FALSE,
  family = "mgaussian",
  lambda =lambda_cand
)

results<-lapply(seq_along(lambda_cand), function(indx){
  lambda_i <- lambda_cand[indx]
  coef_list <- coef(fit_opt, s = lambda_i)
  A_hat<-A_hat(coef_list, K)
  metrix<-metrics(A, A_hat)
  list(
    summary  = data.frame(
    Lambda = lambda_i,
    TypeI = metrix["TypeI"],
    TypeII = metrix["TypeII"],
    Recall = metrix["Recall"],
    Precision = metrix["Precision"],
    F1 = metrix["F1"],
    MSE = metrix["MSE"]
  ),
  A_est=A_hat # we include the estimated matrix as well for separate analysis
  )
  
})
table_results<- lapply(results, `[[`, "summary") %>%
  do.call(rbind, .)
row.names(table_results)<-c("Manual CV", "CV Min","CV 1s.d.", "Criterion df", "Lambda = 0.03")
table_results
#we will print as well all the estimated matrices
results[[1]]$A_est #visualise estiamte with lambda from manual cv
results[[2]]$A_est #visualise estiamte with lambda from cv.glmnet
results[[3]]$A_est #visualise estiamte with lambda 1 s.d. away from the minimum
results[[4]]$A_est #visualise results from the arbitrary larger lambda
A

```
From the table above we notice that between the different hypertuning methods, CV 1 s.d. selects a lambda which is 1 s.d. from the minimum value, therefore leading to a sparser solution. This results in a lower Type I error, while keeping a reasonable Type II error. For this reason, in our monte carlo simulation, we will use this method to select the tuning parameter.

## Monte Carlo Simulation

To do a MC simulation, I have defined a function that create n random seeds that 
will be used to generate different sparse true coefficient matrix. This will then
used to generate various VAR(1) processes. For each replication, we will estimate
the coefficient matrix using LASSO. To optimise the tuning parameter,
we will use the built in function \textit{cv.glmnet}, and select the value that is 1 s.d. away from the minimum. Finally, we will compute the Type I and Type II error rates for 
each replication, and summarize the results across all replications.

For the sake of saving GPU and computational time, I have let run the simulation up to 1000 times. 

```{r}
#define functions that returns 
error_function <- function(tol,coef, tr_coef){  # coef = A_hat, tr_coef = A_true
  S_true <- abs(tr_coef) > 0
  S_est  <- abs(coef)    > tol

  TP <- sum(S_true & S_est)
  FN <- sum(S_true & !S_est)
  FP <- sum(!S_true & S_est)
  TN <- sum(!S_true & !S_est)

  FPR  <- FP/(FP+TN)
  FNR  <-  FN/(FN+TP)
  Prec <-TP/(TP+FP)
  Rec  <-  TP/(TP+FN)
  F1   <- 2*Prec*Rec/(Prec+Rec) 
  
  data.frame(TypeI = FPR, TypeII = FNR, Recall = Rec, Precision = Prec, F1 = F1)
}
  
  
MC_var_1 <- function(K, spar, sd, nu, tol= 5e-3,max_tries = 1000, nrep = 100, 
                     base_seed = 123, T_sim = 1000) {

  seeds <- base_seed + seq_len(nrep) - 1
  
  # storage
  A_list     <- vector("list", nrep)   # store true A per replication
  Ahat_list  <- vector("list", nrep)   # store estimated A_hat
  nu_hat_list <- vector("list", nrep)  # store A_hat - A_true per replication
  
  results_comb <- data.frame(
    TypeI     = numeric(nrep),
    TypeII    = numeric(nrep),
    Recall    = numeric(nrep),
    Precision = numeric(nrep),
    F1        = numeric(nrep)
  )

  for (b in seq_len(nrep)) {
    set.seed(seeds[b])

    # (1) draw a stable sparse A
    A_true <- A_sim(K, spar = spar, sd = sd, max_tries = max_tries)
    tol<-  min(abs(A_true[A_true!=0]))/2
    # (2) simulate VAR(1)
    sigma_var <-sd * diag(1, K, K)
    nu_var    <- rep(nu, length.out = K)
    y_0       <- rep(0, K)
    var_1 <- var_sim(AA = A_true, nu = nu_var, Sigma_u = sigma_var, 
                     nSteps = T_sim, y0 = y_0)

    # (3) create lag matrices
    Y <- coredata(var_1)[-1, ]          # y_t
    Z <- coredata(var_1)[-nrow(var_1),] # y_{t-1}

    # (4) pick lambda via CV
    cv_fit <- cv.glmnet(x = as.matrix(Z), y = as.matrix(Y), intercept=FALSE,
                        family = "mgaussian")
    lam <- cv_fit$lambda.1se  # 1 s.d. away from min

    # (5) fit model and extract A_hat
    fit_best  <- glmnet(x = as.matrix(Z), y = as.matrix(Y), intercept=FALSE,
                        family = "mgaussian", lambda = lam)
    coef_list <- coef(fit_best)
    
    A_est <- matrix(NA_real_, K, K)
    for (i in seq_len(K)) {
      ci <- as.matrix(coef_list[[i]])[-1, 1]   # drop intercept
      A_est[i, ] <- ci
    }

    # (6) compute difference (nu_hat = bias)
    nu_hat <- A_est - A_true

    # (7) metrics
    res_b <- error_function(tol = tol, coef = A_est, tr_coef = A_true)
    results_comb[b, ] <- res_b[1, ]

    # store
    A_list[[b]]     <- A_true
    Ahat_list[[b]]  <- A_est
    nu_hat_list[[b]] <- nu_hat
  }

  # summarize
  summary_df <- data.frame(
    metric = names(results_comb),
    mean   = sapply(results_comb, mean, na.rm = TRUE),
    sd     = sapply(results_comb, sd,   na.rm = TRUE),
    se     = sapply(results_comb, function(x) sd(x, na.rm = TRUE)/sqrt(sum(!is.na(x))))
  )

  # return
  list(
    results = results_comb,
    summary = summary_df,
    A_true  = A_list,
    A_hat   = Ahat_list,
    nu_hat  = nu_hat_list,
    seeds   = seeds
  )
}

```

```{r}
out_5 <- MC_var_1(K = 5, spar = 0.1, sd = 0.1, nu = 0,tol=5e-3, nrep = 1000, T_sim = 1000)
out_5$summary
typeI<-round(out_5$summary[1,2],3)
```
From this simulation in small dimensions, we notice that LASSO has a good performance 
with respect to the Type I error, $\alpha = `r typeI`\%$. 
This implies that LASSO has a low probability of falsely identifying a zero coefficient  as non-zero. 
On the other hand, type II error is high, meaning there is high chance that the model fails to detect a truly non-zero coefficient. Nevertheless, precision remains high,
implying that the relevant coefficients, those on the right upper triangle, are correctly identified most of the time.

### Increasing number of K

We first try with K=10
```{r}
out_10 <- MC_var_1(K = 10, tol=5e-3, spar = 0.1, sd = 0.1, nu = 0, nrep = 1000, T_sim = 1000)
out_10$summary

```
As dimension increase, we notice that the type I error increases steeply, meaning that the model is likely to mistakenly identify a zero ceofficient as non-zero. On top of that, the precision dropped significantly, indicating that a substantial portion of the identified non-zero coefficients are false positives. This suggests that as the dimensionality of the VAR(1) process increases, especially with sparse matrix LASSO struggles to distinguish relevant coefficients diminishes, leading to a higher rate of incorrect selections.


We first try with K=15
```{r}

out_15 <- MC_var_1(K = 15, spar = 0.1, sd = 0.1, nu = 0, tol=5e-3 ,nrep = 1000, T_sim = 1000)
out_15$summary
```
From this simulation with $K = 15$, we notice that the Type I error increases sharply ($62.6%$), meaning that the model often identifies zero coefficients as non-zero. On the other hand, the Type II error remains low ($7.4%$), showing that LASSO is able to detect most of the truly non-zero coefficients. However, precision drops to about $52.5%$, indicating that many of the identified coefficients are false positives. This suggests that in higher dimensions, LASSO becomes less selective, including more irrelevant variables in the model.


