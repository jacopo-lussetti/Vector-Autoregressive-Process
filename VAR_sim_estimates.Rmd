---
title: "Implementation Multiple Time Series Analysis"
author: "Jacopo Lussetti"
date: "2025-04-02"
header-includes:
  - \usepackage{amsmath}
  - \DeclareMathOperator{\VEC}{vec}
  - \usepackage{bm}  # Bold math symbols
output:
  html_document: default
  pdf_document: default
---
```{r, echo=FALSE, warning=FALSE, message=FALSE}
#install required packages
packages <- c("MASS", "zoo", "stats", "rockchalk", "glmnet", "vars", "knitr", "car", "caret", "psych")

for (pckg in packages) {
  if (!require(pckg, character.only = TRUE)) {
    install.packages(pckg)
    library(pckg, character.only = TRUE)
  } else {
    library(pckg, character.only = TRUE)
  }
}



```   

## Companion Matrix function 


```{r}
comp_mtrx <- function(AA){
    ## AA is a K x Kp matrix, so we are able to derive p in the following way
    K <- nrow(AA)
    Kp <- ncol(AA)      
    p <- Kp/K
    
    # Create the empty companion matrix Kp x Kp
    C <- matrix(0, nrow=Kp, ncol=Kp)

    C[1:K,] <- AA
    
    # Add ones on the K-th sub-diagonal
    if (p>1)
        C[(K+1):Kp, 1:(Kp-K)] <- diag(Kp - K)
    return(C)
}
```

## Autocovariance function

Equation (2.1.39) page 29 represents the formula to compute autocovariances of 
a \textit{stable} VAR(p) Process. First and foremost we will then have to evaluate
whether the process is stable 

### Stability check 

A VAR(p) is a \textbf{stable process} if the condition (2.1.9.) holds: 

$$
det(I- \mathbf{A}z) \neq 0 \ if \ |z|<1 \ \text{where z are eigenvalues for A}
$$

```{r}
  var_roots<-function(AA){
    if(nrow(AA)==ncol(AA)){  # matrix is squared
      C<-AA  
    }else{
      C<-comp_mtrx(AA) # transform form compact matrix into companion matrix 
    }
    eig<-eigen(C)$values
    return(eig) 
  }
  
```

### Formula
After evaluating the conditions for stability, we proceed then defining the 
formula to compute contrivances. Th


\begin{equation}
  \VEC \ \Gamma_Y(0)= (I_{(Kp)^2}- \mathbf{A} \otimes \mathbf{A})^{-1} \VEC \ \Sigma_U
\end{equation}


```{r}

autocov_fun<-function(A, Sigma_u,p=1){ # A for high-order var is combined matrix,
  K<-nrow(Sigma_u) 
  Kp<-K * p
  #for var(1) is just A1
 if(p>1){
   #compute companion
   A<- comp_mtrx(A)
   #extend original sigma_u
  Sigma_U<-matrix(0, nrow=Kp, ncol=Kp) 
  Sigma_U[1:K, 1:K]<-Sigma_u
   
 }else{
   Sigma_U<-Sigma_u
 } 
 # compute the Kronecker product
  I<-diag(1, Kp^2)
  #compute vectorised Sigma_U
  vec_Sigma_U<-as.vector(Sigma_U)
  # compute the Autocovariance function
  vec_gamma_0<-solve(I - kronecker(A, A)) %*% vec_Sigma_U
  # reshape the result into a matrix
  Gamma_Y_0<-matrix(vec_gamma_0, nrow=Kp, ncol=Kp)
  
  return(Gamma_Y_0)  
}
```
## Equilibrium Points

Equilibrium points are defined by formula 2.1.10 at page 16

\begin{equation}
  \mathbf{\mu}:= E(Y_t)= (I_{Kp}-\mathbf{A})^{-1} \mathbf{\nu}
\end{equation}

```{r}
equilibrium<-function(A, nu){
  #check stability condition
  eig<-var_roots(A)
  if(any(Mod(eig)>=1)){   
      stop("Trajectories are not stable")
  }
  Kp<-nrow(A)
  I_Kp<-diag(1,Kp)
  values<-solve(I_Kp-A) %*% nu
  return(values)
}
```



## VAR(p) model

For this case we just consider the function simulating the trajectories, whereas
equilibrium and autocovariance functions are treated separately. 

```{r}
  var_sim <- function(AA, nu, Sigma_u, nSteps, y0) {
    K <- nrow(Sigma_u)
    Kp <- ncol(AA)
    p <- Kp/K
        
    if (p > 1) {
        C <- comp_mtrx(AA) # form the  companion matrix of the var(p) process
    } else {
        C <- AA  
    }
    y_t <- matrix(0, nrow =  nSteps, ncol=Kp) #trajectories matrix nSteps x Kp
    y_t[1, 1:Kp] <- y0 #add initial value to initiate the simulation
    noise <- mvrnorm(n = nSteps, mu = rep(0, K), Sigma = Sigma_u) #assuming that 
    #residuals follow a multivariate normal distribution    
    
    for (t in 2:nSteps) {
        y_t[t, ] <- C %*% y_t[t-1, ]
        y_t[t, 1:K] <- y_t[t, 1:K] + nu + noise[t,]
    }
    
    y_t <- zoo(y_t[,1:K], 1:nSteps)  
    return(y_t)
  }
```



# Estimates

## Estimates of coefficents
We first define a formula to compute Z values, which are key parameters for 
both estimating coefficient \& auto-correlation matrix. 

```{r}


par_estimate <- function(y_t, p=1) {
  nObs <- nrow(y_t)  # Number of observations
  K <- ncol(y_t)  # Number of variables
  T <- nObs - p  # Number of usable observations

  # y 
  Y <- y_t[(p + 1):nObs, ]  # T x K matrix

  # Z
  Z <- matrix(1, nrow = T, ncol = (K * p + 1))  # Intercept + lagged values

  for (i in 1:p) {
    col_start <- 2 + (i - 1) * K
    col_end <- 1 + i * K
    Z[, col_start:col_end] <- y_t[(p + 1 - i):(nObs - i), ]
  }

  # Estimate coefficients using OLS: B_hat = (Z'Z)^(-1) Z'Y
  B_hat <- solve(t(Z) %*% Z) %*% t(Z) %*% Y  # (K*p + 1) x K matrix

  return(list(
    Y = Y,
    Z = Z,
    B_hat = B_hat,  # Estimated VAR parameters
    T = T
  ))
}

```

```{r}
estimator<-function(Y, Z, p=1, method = c("standard", "qr", "lsfit")) {

    # Estimator
     method <- match.arg(method)
     if(method == "standard"){
        B_hat <- solve(t(Z) %*% Z, t(Z) %*% Y)
        B_hat <- t(B_hat)
     } else if(method == "qr"){
        qr_decomp <- qr(Z)  
        B_hat <- qr.coef(qr_decomp, Y)
     } else if(method == "lsfit"){
       fit <- lsfit(Z, Y) 
        B_hat <- fit$coef  
     } else {
        stop("Unknown method")
     }
    return(list(nu=B_hat[,1], AA=B_hat[,-1]))
  }

```


## Estimates of the autocovariance function

```{r}
est_autocov <- function(y_t, Y, Z, T, p=1){
    K <- ncol(y_t)
    Kp <- K * p
    I_t <- diag(T)

    # QR decomposition of Z to avoid singularity issues
    qr_decomp <- qr(Z)
    Q <- qr.Q(qr_decomp)
    P_Z <- Q %*% t(Q)  # Projection matrix

    # Compute bias-corrected covariance
    bias_sigma <- 1/T * t(Y) %*% (I_t - P_Z) %*% Y

    # Degrees of freedom correction
    d.f. <- T / (T - Kp - 1)
    unbiased <- d.f. * bias_sigma  # Corrected covariance estimate

    return(unbiased)
}

```
  
# Test A


```{r, fig.width=7, fig.height=5}
set.seed(123)
  p<-1
  A<-matrix(c(0.5, 0.1, 0., 0., 0.1, 0.2, 0., 0.3, 0.3), nrow=3, ncol=3)
  nu<-matrix(c(0.05, 0.02, 0.04), nrow=3)
  Sigma_u <- matrix(c(1.0, 0.2, 0.3, 0.2, 2.0, 0.5, 0.3, 0.5, 3.0), nrow = 3, ncol = 3)
  nSteps <- 200
  y0 <- matrix(c(0.1, -0.1, 0.2),ncol=ncol(A))

#compute trajectories
y_t_a<-var_sim(A, nu, Sigma_u, nSteps, y0)  
plot(y_t_a, main = "VAR(1) Simulation", xlab = "Time", ylab = "y values", col = 1:3, lty = 1)
```
## Multivariate Least Squares Estimators


```{r}
#estimate parameters
par_A<-par_estimate(y_t_a)
#estimate coefficient 

test_A<-estimator(par_A$Y, par_A$Z)
auto_cov_A<-est_autocov(y_t_a, par_A$Y, par_A$Z, par_A$T)

```
Now we will compare original input for the simulation \& estimated values

Coefficient Matrix
```{r}
A_true <- A  
A_est <- test_A$AA 

diff_A <- A_true - A_est  
print(diff_A)

```
Intercept Matrix
```{r}
nu_true <- nu  # True nu from the simulation
nu_est <- test_A$nu  # Estimated nu

diff_nu <- nu_true - nu_est  # Compute the difference
print(diff_nu)
```
Covariance Matrix

```{r}
Sigma_u_true <- Sigma_u  # True covariance matrix
Sigma_u_est <- auto_cov_A  # Estimated autocovariance

diff_Sigma_u <- Sigma_u_true - Sigma_u_est  # Compute the difference
print(diff_Sigma_u)

```
# Test B

Kp-dimensional representation for VAR(p) is defined as following:

\begin{align*}
Y_t &= \nu + \mathbf{A} Y_{t-1} + U_t \\
\text{where} \quad
Y_t &:= 
\begin{bmatrix}
y_t \\
y_{t-1} \\
\vdots \\
y_{t-p+1}
\end{bmatrix}, \quad
\nu :=
\begin{bmatrix}
\nu \\
0 \\
\text{($K p \times 1$ zeros)}
\end{bmatrix}, \\
\mathbf{A} &:=
\begin{bmatrix}
A_1 & A_2 & \ldots & A_{p-1} & A_p \\
I_K & 0 & \ldots & 0 & 0 \\
0 & I_K & \ldots & 0 & 0 \\
\vdots & & \ddots & \vdots & \vdots \\
0 & 0 & \ldots & I_K & 0
\end{bmatrix}, \quad
U_t :=
\begin{bmatrix}
u_t \\
0 \\
\vdots \\
0
\end{bmatrix}
\end{align*}

We first start with the simulation
```{r, fig.width=7, fig.height=5}
# Define parameters for Test B
set.seed(123)
p_B <- 2  # Number of lags for VAR(p)
nSteps_B <- 100
K_B <- 2  # Number of variables (size of y_t)
Sigma_u_B <- matrix(c(2, 0.3, 0.3, 3), nrow = 2, ncol = 2)
y0_B <- c(0.5, 2, 1, 5) 
nu_int_B <- matrix(c(0.5, 0.9), nrow = 2)

A_1 <- matrix(c(0.5, 0.4, 0.1, 0.5), nrow = 2, ncol = 2)
A_2 <- matrix(c(0, 0.25, 0, 0), nrow = 2, ncol = 2)
AA <- cbind(A_1, A_2)

# Simulate time series for Test B
y_t_B <- var_sim(AA, nu_int_B, Sigma_u_B, nSteps_B, y0_B)
plot(y_t_B, main = "VAR(2) Simulation", xlab = "Time", ylab = "y values", col = 1:2, lty = 1)

```
and then estimates

```{r}
# Estimate the parameters for Test B
par_B <- par_estimate(y_t_B, p = p_B)

# Estimate coefficients
test_B <- estimator(par_B$Y, par_B$Z)

# Display estimated coefficients for Test B
A_est_B <- test_B$AA
nu_est_B <- test_B$nu

# Estimate autocovariance matrix for Test B
auto_cov_B <- est_autocov(y_t_B, par_B$Y, par_B$Z, par_B$T, p = p_B)

# Display the estimated autocovariance matrix
auto_cov_B

```
Coefficient Matrix

```{r}
# True and estimated coefficient matrix for Test B
A_true_B <- AA  # True coefficient matrix
diff_A_B <- A_true_B - A_est_B  # Difference between true and estimated A

print("Difference in coefficient matrix (A):")
print(diff_A_B)

```
Intercept Matrix

```{r}
# True and estimated intercept matrix for Test B
nu_true_B <- nu_int_B  # True intercept vector
diff_nu_B <- nu_true_B - nu_est_B  # Difference between true and estimated nu

print("Difference in intercept matrix (nu):")
print(diff_nu_B)

```
Covariance Matrix

```{r}
# True and estimated covariance matrix for Test B
Sigma_u_true_B <- Sigma_u_B  # True covariance matrix
diff_Sigma_u_B <- Sigma_u_true_B - auto_cov_B  # Difference between true and estimated covariance

print("Difference in covariance matrix (Sigma_u):")
print(diff_Sigma_u_B)


```

# VAR(1) simulation with sparese coefficient Matrix

\begin{enumerate}
  \item \textbf{VAR(1) Simulation:} \\
  Generate data from the process
  \[
    y_t = A y_{t-1} + u_t, \quad u_t \sim \mathcal{N}(0, \sigma^2 I).
  \]
  where A is a lower triangular matrix with sparsity on the top right. 
  \item \textbf{Estimation via LASSO:} \\
  Apply LASSO regression to estimate the autoregressive matrix $A$ from the simulated data through R funtion \textit{glmnet}.
  \item \textbf{Optimisation of the tuning parameter} \\
    Through cross-validation or information criterion, and we compare strategies
    \item  \textbf{Monte Carlo Repetition}
  \item \textbf{Evaluation of Errors:} \\
  compute Type I and Type II error rates in detecting zero vs.~nonzero coefficients.
  \item \textbf{Increase dimension}\\
  We try larger matrices by increasing K, and we observe how LASSO’s performance changes as dimension increases.
\end{enumerate}

## VAR(1) Simulation

```{r}
set.seed(1234)
stab_test <- function(kp, A, tol = 1e-8)
{
  if (!is.matrix(A) || nrow(A) != ncol(A)) {
    stop("The matrix is not square")
  }
  eig <- eigen(A, only.values = TRUE)$values  # computing the eigenvalues
  
  for (i in 1:length(eig)) {     
    if (Mod(eig[i]) >= 1 - tol) { # Mod also handles complex numbers
      return(FALSE)               # <-- fixed typo "returm"
    }
  }
  return(TRUE)
}


A_sim <- function(K, spar, sd, max_tries = 1000){
  tries <- 0 
  repeat{
    A <- matrix(0L, K, K)
    idx_up <- which(upper.tri(A))
    n_up   <- length(idx_up)

    if (n_up > 0) {
      A[idx_up] <- rnorm(n_up, mean = 0, sd = sd)   #fill the upper 
  #triangle with r.v. from a normal distribution
  #cancel a certain percentage of the elements
      nmiss <- round(n_up * spar)
      if (nmiss > 0) { # add sparsity
        zero_idx <- sample(idx_up, nmiss, replace = FALSE)
        A[zero_idx] <- 0
      }
    }

    # we apply the formula previously defined to check stability
    if (stab_test(K, A) == TRUE) return(A)
     # else try again
    tries <- tries + 1
    #to prevent infinite loop due to using repeat loop, we set a max number of
    #iterations
    if (tries >= max_tries) {
      stop("Could not generate a stable A within max_tries.")
    }
  }
}

#now we generate a stable upper triangle coef matrix with sparsity 
K <- 5 
sd_var<-0.1
A <- A_sim(K, spar = 0.1, sd = sd_var, max_tries = 10) # moderate sparsity, modest sd A
stab_test(K,A)
```
Now we are able to generate a VAR(1) from the function specified at page 3

```{r}

sigma_var<- diag(1,5,5)* sd_var
nu_var <- rep(0, K)  
y_0 <- rep(0, K)  
T_sim<-10000
var_1<- var_sim(AA=A,nu=nu_var,Sigma_u=sigma_var, nSteps=T_sim, y0=y_0 )
```


# Estimation via LASSO

In this step we will apply lasso regression by applying the packages \textit{glmnet}

```{r}


# Build lagged design (Z) and aligned response (Y)
Z <- coredata(lag(var_1, k = -1))   # predictors y_{t-1}
Y_full <- coredata(var_1)                # responses  y_t

# Drop first row to align (remove the NA created by lag)
Y <- Y_full[-1, , drop = FALSE]

# If univariate, coerce to matrix to allow 2D subsetting later
if (is.null(dim(Z))) Z <- matrix(Z, ncol = 1)
if (is.null(dim(Y))) Y <- matrix(Y, ncol = 1)

# 70/30 split 
n <- nrow(Z)
cut_off <- round(0.70 * n)
X_train <- Z[1:cut_off, , drop = FALSE]
Y_train <- Y[1:cut_off, , drop = FALSE]
X_test  <- Z[(cut_off + 1):n, , drop = FALSE]
Y_test  <- Y[(cut_off + 1):n, , drop = FALSE]

# Manual CV over a simple lambda grid (avoid 0 exactly)
lambdas <- seq(0,1, 0.001)
val_error <- data.frame(lambda = lambdas, error = NA_real_)

for (i in seq_along(lambdas)) {
  lmbd <- lambdas[i]
  fit  <- glmnet(X_train, Y_train, family = "mgaussian", lambda = lmbd)
  pred <- predict(fit, newx = X_test, s = lmbd)[,,1]
  mse_per_series   <- colMeans((Y_test - pred)^2)
  val_error$error[i] <- mean(mse_per_series)
}

best_lambda_1 <- val_error$lambda[which.min(val_error$error)]
best_lambda_1
```
We try with default cross-validation from glmnet

```{r}
cv_fit <- cv.glmnet(
  x = as.matrix(Z),
  y = as.matrix(Y),
  family = "mgaussian"  # var us a mutlivariate regression
)

plot(cv_fit)
best_lambda_2 <- cv_fit$lambda.1se # lambda.min brings results that are 
#far not optimal, and have higher type I error
best_lambda_2
```




```{r}
fit_opt<-glmnet(
  x=as.matrix(Z),
  y = as.matrix(Y),
  family = "mgaussian",
  lambda = c(best_lambda_1,best_lambda_2,0.03)
)
A_hat_114<-coef(fit_opt, s=best_lambda_1)
A_hat_129<- coef(fit_opt, s = best_lambda_2)
A_hat_arb<- coef(fit_opt, s =0.03) #arbitrary lambda to see how results change
#we merge estimates in unique A_hat
A_hat_matrix_1<-matrix(NA, nrow=K, ncol=K)
for(i in 1:5){
  ci <- as.matrix(A_hat_114[[i]])           # ith response
lag_rows <- rownames(ci) != "(Intercept)"  # drop intercept
A_hat_matrix_1[i, ] <- ci[lag_rows, 1]
}
A_hat_matrix_1

A_hat_matrix_2<-matrix(NA, nrow=K, ncol=K)
for(i in 1:5){
ci <- as.matrix(A_hat_129[[i]])           # ith response
lag_rows <- rownames(ci) != "(Intercept)"  # drop intercept
A_hat_matrix_2[i, ] <- ci[lag_rows, 1]
}
A_hat_matrix_arb<-matrix(NA, nrow=K, ncol=K)
for(i in 1:5){
ci <- as.matrix(A_hat_arb[[i]])           # ith response
lag_rows <- rownames(ci) != "(Intercept)"  # drop intercept
A_hat_matrix_arb[i, ] <- ci[lag_rows, 1]
}
A_hat_matrix_arb
A
```
Now we check 


```{r}

# --- Support function
tol <- 1e-3
S_true  <- abs(A)                 > tol
S_est_1 <- abs(A_hat_matrix_arb)    > tol
S_est_2 <- abs(A_hat_matrix_2)    > tol

# --- Confusion counts 
TP_1 <- sum(S_true & S_est_1)
FN_1 <- sum(S_true & !S_est_1)
FP_1 <- sum(!S_true & S_est_1)
TN_1 <- sum(!S_true & !S_est_1)

TP_2 <- sum(S_true & S_est_2)
FN_2 <- sum(S_true & !S_est_2)
FP_2 <- sum(!S_true & S_est_2)
TN_2 <- sum(!S_true & !S_est_2)

# --- Rates (guard divisions) ---
FPR_1 <- if ((FP_1+TN_1)>0) FP_1/(FP_1+TN_1) else NA_real_
FPR_2 <- if ((FP_2+TN_2)>0) FP_2/(FP_2+TN_2) else NA_real_

FNR_1 <- if ((FN_1+TP_1)>0) FN_1/(FN_1+TP_1) else NA_real_
FNR_2 <- if ((FN_2+TP_2)>0) FN_2/(FN_2+TP_2) else NA_real_

Prec_1 <- if ((TP_1+FP_1)>0) TP_1/(TP_1+FP_1) else NA_real_
Prec_2 <- if ((TP_2+FP_2)>0) TP_2/(TP_2+FP_2) else NA_real_

Rec_1  <- if ((TP_1+FN_1)>0) TP_1/(TP_1+FN_1) else NA_real_
Rec_2  <- if ((TP_2+FN_2)>0) TP_2/(TP_2+FN_2) else NA_real_

f1_1 <- if (!is.na(Prec_1+Rec_1) && (Prec_1+Rec_1)>0) 2*Prec_1*Rec_1/(Prec_1+Rec_1) else NA_real_
f1_2 <- if (!is.na(Prec_2+Rec_2) && (Prec_2+Rec_2)>0) 2*Prec_2*Rec_2/(Prec_2+Rec_2) else NA_real_
#compute MSE 
diff_matrix_1<- A - A_hat_matrix_1
diff_matrix_2<- A - A_hat_matrix_2
#apply the Frobenius norm squared
MSE_1<- 1/(K^2) *tr(t(diff_matrix_1) %*% diff_matrix_1)
MSE_2<- 1/(K^2) *tr(t(diff_matrix_2) %*% diff_matrix_2)


# Result Tables
results <- data.frame(
  Model     = c(paste0("Lambda=", 0.03), paste0("Lambda=",best_lambda_2)),
  TypeI     = c(FPR_1, FPR_2),
  TypeII    = c(FNR_1, FNR_2),
  Recall    = c(Rec_1, Rec_2),
  Precision = c(Prec_1, Prec_2),
  F1        = c(f1_1, f1_2),
  MSE       =c(MSE_1, MSE_2)
)
results
```
From the results we can notice that built-in cross-validation from glmnet is not
effectively able to capture the sparsity pattern of the true matrix A. Just by 
applying an arbitrary larger lambda, we are able to reduce significantly the type I error. It is then crucial to find a better method to optimise the tuning parameter.
\cite{HIROSE201328} proposes a method based on Mallows $C_p$ criteria, which it works
as follows:
\begin{table}[!h]
\centering
\caption{Summary of model selection criteria based on degrees of freedom}
\label{tab:criteria-df}
\begin{tabular}{ll}
\hline
\textbf{Criterion} & \textbf{Formula} \\
\hline
$C_p$   & $\displaystyle \|y - \hat{\mu}\|^{2} + 2\,\tau^{2}\,df$ \\
AIC     & $\displaystyle N\log(2\pi\tau^{2}) + \frac{\|y - \hat{\mu}\|^{2}}{\tau^{2}} + 2\,df$ \\
AICc    & $\displaystyle N\log\!\Big(2\pi\,\frac{\|y - \hat{\mu}\|^{2}}{N}\Big) + \frac{N - 2N\,df}{\,N - df - 1\,}$ \\
BIC     & $\displaystyle N\log(2\pi\tau^{2}) + \frac{\|y - \hat{\mu}\|^{2}}{\tau^{2}} + (\log N)\,df$ \\
GCV     & $\displaystyle \frac{1}{N}\,\frac{\|y - \hat{\mu}\|^{2}}{(1 - df/N)^{2}}$ \\
\hline
\end{tabular}
\end{table}
When we have normal residuals and no sparsity, the d.f can be computed as follows:
\begin{align*}
d.f. &= \Sigma_{i=1}^N \frac{\text{cov}(\hat{\mu}_i, y_i)}{\sigma^2}\\
     & \text{tr}(X(X^TX)^{-1}X^T) = \\
     & \text{rank}(X)
\end{align*}
However, in penalised likelihood methods, d.f. depends on the tuning parameter $\lambda$.
First and foremost we are gonna compute the \textit{Generalised path seeking algorithm}
to derive the number of degree of freedoms from chapter 3 of \cite{HIROSE201328}.

The solution of the penalised regression at the tuning paramter \textit{t} is denoted as $\hat{\beta}(t)$. We first derive the updating formula for small change in the tuning parameter, which by theory can be continous. 
\begin{equation}
  \hat
\end{equation}


Discrimination of lambda based on the mellow $C_p$ criteria, BIC and AIC 
```{r}
criteria_df<-function(X,Y, p,steps){
  n<-nrow(X)
  #recall that the min lambda for gaussian is C'sigma sqrt(log(p)/n)
  C_i <- apply(X, 2, function(col) sqrt(sum(col^2)) / sqrt(n))
  C <- max(C_i)
    #we compute an initial estimator of sigma from the OLS 
  beta_ols <- solve(t(X) %*% X) %*% t(X) %*% Y
  resid <- Y - X %*% beta_ols
  sigma_hat <- sqrt(sum(resid^2) / (n - p))
  lambda_0<- C * sigma_hat * sqrt(log(p) / n) # initial lambda 
   #create a seq of candidate lambda with increments = steps
  lambda_seq<-seq(lambda_0, 3*lambda_0, by=steps)
  len_lambda<-length(lambda_seq)
  #create a data frame to store the results
  results<-data.frame(
   lambda_seq=lambda_seq,
   df=as.numeric(rep(0, len_lambda)),
   Cp=as.numeric(rep(0, len_lambda)),
   AIC=as.numeric(rep(0, len_lambda)),
   BIC=as.numeric(rep(0, len_lambda))
  )
  #we compute now the compute estimate by using glmnet 
  for(i in 1:length(lambda_seq)){
    fit<-glmnet(
      X,
      Y,
      family = "mgaussian",
      lambda = lambda_seq[i]
    )
    #extract df
    df<-fit$df
    #compute sd
   
    Y_hat<-predict(fit, newx=X)
    resid<-Y - Y_hat[,,1]
    sigma_hat<-sqrt(sum(resid^2)/(n-df))
    #compute mu hat
    for(i in 1:p){
      betas<-fit$beta[[i]]
      results$Cp[i]<-sum(Y[,i]- X[,i]* betas)^2 + 2*sigma_hat^2 * df
      results$AIC[i]<-n*log(2*pi*sigma_hat^2) + sum((Y[,i]- X[,i]* betas)^2)/sigma_hat^2 + 2*df
      results$BIC[i]<-n*log(2*pi*sigma_hat^2) + sum((Y[,i]- X[,i]* betas)^2)/sigma_hat^2 + log(n)*df
    }
  }
    return(results)
    }
test<-criteria_df(Z, Y, p=5, steps=0.01)
```
\textbf{Generalised path seeking algorithm}\\

Experimental Procedure
```{r}
gps<-function(X,Y,p, tol,steps){
  
  n<-nrow(X)
  p<-ncol(X)
  T<-n-p 
  #recall that the min lambda for gaussian is C'sigma sqrt(log(p)/n)
  C_i<-seq(0,p)
  for(i in 1:p){
    C[i]<-colSums(X[,i])/(sqrt(n))
    
  }
  C<-max(C_i)
  #we compute an initial estimator of sigma from the OLS 
  beta_ols <- solve(t(X) %*% X) %*% t(X) %*% Y
  resid <- Y - X %*% beta_ols
  sigma_hat <- sqrt(sum(resid^2) / (n - p))
  lambda_0<- C * sigma_hat * sqrt(log(p) / n) # initial lambda 
   #create a seq of candidate lambda with increments = steps
  lambda_seq<-seq(lambda_0, 2*lambda_0, by=steps)
  #We create a data frame to store the criterions
  Criterions<-data.frame(
    lambda_seq=lambda_seq,
    df=as.numeric(rep(0, len_lambda)),
    Cp=as.numeric(rep(0, len_lambda)),
    AIC=as.numeric(rep(0, len_lambda)),
    BIC=as.numeric(rep(0, len_lambda))
  )
  len_lambda<-length(lambda_seq)
  #we compute now the compute estimate by using glmnet 
  for(i in 1:length(len_lambda)){
    fit<-glmnet(
      X=as.matrix(X),
      Y=as.matrix(Y),
      family = "mgaussian",
      lambda = lambda_seq[i]
    )
    #extract df
    df<-fit$df
    Mellow_c<-
  }
  
}
```


\textbf{Mellow $C_p$ criteria}
```{r}


```
## Monte Carlo Simulation
```{r}
#define functions that returns 
error_function <- function(tol, coef, tr_coef){  # coef = A_hat, tr_coef = A_true
  S_true <- abs(tr_coef) > tol
  S_est  <- abs(coef)    > tol

  TP <- sum(S_true & S_est)
  FN <- sum(S_true & !S_est)
  FP <- sum(!S_true & S_est)
  TN <- sum(!S_true & !S_est)

  FPR  <- FP/(FP+TN)
  FNR  <-  FN/(FN+TP)
  Prec <-TP/(TP+FP)
  Rec  <-  TP/(TP+FN)
  F1   <- 2*Prec*Rec/(Prec+Rec) 
  
  data.frame(TypeI = FPR, TypeII = FNR, Recall = Rec, Precision = Prec, F1 = F1)
}
  
  
MC_var_1 <- function(K, spar, sd, nu, max_tries = 1000, nrep = 100, 
                     base_seed = 123, T_sim = 1000, tol = 1e-3) {

  seeds <- base_seed + seq_len(nrep) - 1

  # storage
  A_list     <- vector("list", nrep)   # store true A per replication
  Ahat_list  <- vector("list", nrep)   # store estimated A_hat
  nu_hat_list <- vector("list", nrep)  # store A_hat - A_true per replication
  
  results_comb <- data.frame(
    TypeI     = numeric(nrep),
    TypeII    = numeric(nrep),
    Recall    = numeric(nrep),
    Precision = numeric(nrep),
    F1        = numeric(nrep)
  )

  for (b in seq_len(nrep)) {
    set.seed(seeds[b])

    # (1) draw a stable sparse A
    A_true <- A_sim(K, spar = spar, sd = sd, max_tries = max_tries)

    # (2) simulate VAR(1)
    sigma_var <- diag(1, K, K)
    nu_var    <- rep(nu, length.out = K)
    y_0       <- rep(0, K)
    var_1 <- var_sim(AA = A_true, nu = nu_var, Sigma_u = sigma_var, 
                     nSteps = T_sim, y0 = y_0)

    # (3) create lag matrices
    Y <- coredata(var_1)[-1, ]          # y_t
    Z <- coredata(var_1)[-nrow(var_1),] # y_{t-1}

    # (4) pick lambda via CV
    cv_fit <- cv.glmnet(x = as.matrix(Z), y = as.matrix(Y), family = "mgaussian")
    lam <- cv_fit$lambda.1se

    # (5) fit model and extract A_hat
    fit_best  <- glmnet(x = as.matrix(Z), y = as.matrix(Y), 
                        family = "mgaussian", lambda = lam)
    coef_list <- coef(fit_best)
    
    A_est <- matrix(NA_real_, K, K)
    for (i in seq_len(K)) {
      ci <- as.matrix(coef_list[[i]])[-1, 1]   # drop intercept
      A_est[i, ] <- ci
    }

    # (6) compute difference (nu_hat = bias)
    nu_hat <- A_est - A_true

    # (7) metrics
    res_b <- error_function(tol = tol, coef = A_est, tr_coef = A_true)
    results_comb[b, ] <- res_b[1, ]

    # store
    A_list[[b]]     <- A_true
    Ahat_list[[b]]  <- A_est
    nu_hat_list[[b]] <- nu_hat
  }

  # summarize
  summary_df <- data.frame(
    metric = names(results_comb),
    mean   = sapply(results_comb, mean, na.rm = TRUE),
    sd     = sapply(results_comb, sd,   na.rm = TRUE),
    se     = sapply(results_comb, function(x) sd(x, na.rm = TRUE)/sqrt(sum(!is.na(x))))
  )

  # return
  list(
    results = results_comb,
    summary = summary_df,
    A_true  = A_list,
    A_hat   = Ahat_list,
    nu_hat  = nu_hat_list,
    seeds   = seeds
  )
}

  out_5 <- MC_var_1(K = 5, spar = 0.3, sd = 0.1, nu = 0, nrep = 1000, T_sim = 1000)
  out_5$summary

```
# Increasing number of K

We first try with K=10
```{r}
out_10 <- MC_var_1(K = 10, spar = 0.3, sd = 0.1, nu = 0, nrep = 1000, T_sim = 1000)
out_10$summary

```

We first try with K=15
```{r}

out_15 <- MC_var_1(K = 15, spar = 0.3, sd = 0.1, nu = 0, nrep = 1000, T_sim = 1000)
out_15$summary
```

# Other methods of hypertuning $\lambda$
##Mellow w's $C_p$ criteria

We try to compute other methods to optimise the tuning parameter. Built-in 
packages tends to work for classification models, and does not work well for
time series. We then try to implement a manual k-fold cross-validation. An example
is \cite{HIROSE201328}, which proposes to use Mallows $C_p$ criteria to hypertuning.

```{r}

cp_tune_glmnet_mgaussian <- function(X, Y, lambdas, standardize = TRUE) {
  stopifnot(nrow(X) == nrow(Y))
  n <- nrow(X); K <- ncol(Y)

  # 1) Fit once over the full lambda grid
  fit <- glmnet(x = as.matrix(X),
                y = as.matrix(Y),
                family = "mgaussian",
                lambda = lambdas,
                standardize = standardize,
                intercept = TRUE)

  # helper: extract KxP coefficient *per response* and count nonzeros (no intercept)
  coef_list_at <- function(fit, s) {
    co <- coef(fit, s = s)           # list of length K
    # each co[[i]] is a dgCMatrix with rows: (Intercept, predictors...)
    co
  }
  df_count <- function(co_list) {
    # count non-zeros across all responses, excluding intercept row
    sum(vapply(co_list, function(M) {
      as.integer(sum(abs(M[-1, 1, drop = FALSE]) > 0))
    }, integer(1L)))
  }

  # 2) Residual sums of squares on TRAIN for each lambda
  #    (Frobenius norm across all series)
  rss_at <- function(s) {
    pred <- predict(fit, newx = as.matrix(X), s = s)[,,1]  # n x K
    sum((Y - pred)^2)
  }

  # 3) Estimate tau^2 (sigma^2) from the *most complex* model (smallest lambda)
  s_full <- min(lambdas)
  rss_full <- rss_at(s_full)
  df_full  <- df_count(coef_list_at(fit, s_full))
  # total params counted: df_full (slopes) + K (intercepts)
  denom <- n * K - (df_full + K)
  tau2_hat <- if (denom > 0) rss_full / denom else rss_full / (n * K)  # ML fallback

  # 4) Compute Cp (and friends) for every lambda
  Cp   <- numeric(length(lambdas))
  AICc <- numeric(length(lambdas))
  BIC  <- numeric(length(lambdas))
  GCV  <- numeric(length(lambdas))

  for (i in seq_along(lambdas)) {
    s  <- lambdas[i]
    rs <- rss_at(s)
    df <- df_count(coef_list_at(fit, s))

    # Mallows Cp (multivariate: use total RSS and same df notion)
    Cp[i] <- rs + 2 * tau2_hat * df

    # Information criteria (optional)
    # log-likelihood under spherical errors (up to constants):
    #  -2 log L ≈ (n*K) * log(2*pi*tau^2) + RSS / tau^2
    # Use tau2_hat as plug-in; comparisons are consistent up to constants.
    npar <- df + K  # slopes + intercepts
    aic_like <- (n*K) * log(2*pi*tau2_hat) + rs / tau2_hat
    BIC[i]  <- aic_like + log(n*K) * npar
    # small-sample corrected AIC (AICc); use total "sample size" n*K
    AICc[i] <- aic_like + 2*npar + (2*npar*(npar+1)) / max(1, (n*K - npar - 1))
    # GCV
    GCV[i]  <- (rs / (n*K)) / (1 - df/(n*K))^2
  }

  best_idx <- which.min(Cp)
  list(
    lambda_star = lambdas[best_idx],
    Cp = Cp, AICc = AICc, BIC = BIC, GCV = GCV,
    tau2_hat = tau2_hat,
    df = vapply(lambdas, function(s) df_count(coef_list_at(fit, s)), integer(1L)),
    fit = fit
  )
}

#we run the code using the previous splitted dataset
#we first compute lambas from the package glmnet

fit_lambdas<-glmnet(
  x=as.matrix(Z),
  y = as.matrix(Y),
  family = "mgaussian"
)
lambdas_grid <- fit_lambdas$lambda
lambdas_grid
#now we implement the mallow C_p criteria

cp_tune <- cp_tune_glmnet_mgaussian(X = Z, Y = Y, lambdas = lambdas_grid)
```
