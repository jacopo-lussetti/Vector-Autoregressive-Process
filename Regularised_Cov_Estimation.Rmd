---
title: "Regularised Cov. Estimation"
author: "Jacopo Lussetti"
date: "`r Sys.Date()`"
output:
  pdf_document:
    citation_package: natbib
  html_document:
    citation_package: natbib
headers-includes:
- \usepackage{amsmath}
- \usepackage{amssymb}
bibliography: reference.bib
---
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(MASS)
library(zoo)
library(stats)
library(vars)
library(glmnet)
library(ggplot2)
library(tidyr)
library(dplyr)
library(latex2exp) #write expression with latex in ggplot
library(RSpectra)
library(gridExtra)
library(Matrix)
```

This report is aiming at replicating the methodology by @basu2015regularized
on estimating covariance matrices and their precision matrices for high-dimensional data.

Previous studies on the covariance estimator(@chen2013covandprecision)

# Exercise 1 

#Data Generation
To simulate the stochastic regression model, we will generate data from our code
that allows to randomly generate an uppper triangle coefficient matrix. Then we
will simulate the data with a VAR(1) model. Then we will proceed to use a 
Gaussian VAR(1) model \\ 

$$
X^t=AX^{t-1} + \epsilon^t, \quad \epsilon^t \overset{\text{i.i.d.}}{\sim}\mathcal{N}(0, \sigma^2 I_{Kp}), \quad \text{and}
\quad \text{diag}(A) =0.2
$$
We first define a series of equations that we will use throughout the simulation.
To ensure that \mathcal{l}2-norm of the coefficient matrix is equal to a target value, we apply the following procedure.


```{r}
#we define first of all the various functions
##Simulation upper triangular matrix
### To ensure that process is stable, we need to check the abs of eigenvalue
stab_test <- function(kp, A, tol = 1e-8)
{
  if (!is.matrix(A) || nrow(A) != ncol(A)) {
    stop("The matrix is not square")
  }
  eig <- eigen(A, only.values = TRUE)$values  # computing the eigenvalues
  
  for (i in 1:length(eig)) {     
    if (Mod(eig[i]) >= 1 - tol) { 
      return(FALSE)               
    }
  }
  return(TRUE)
}


#function to compute companion matrix
comp_mtrx <- function(AA,p ){ #AA is the cbind of covariate matrices
  K<-nrow(AA)
  Kp<-K*p
  C<-matrix(0, nrow=Kp, ncol=Kp)
  C[1:K,1:Kp]<-AA
  C[(K+1):Kp,1:(K*(p-1))]<- diag(1,nrow=K, ncol=K )
  return(C)
 }

#
  est_autocov <- function(y_t, Y, Z, T, p=1){
    K <- ncol(y_t)
    Kp <- K * p
    I_t <- diag(T)

    # QR decomposition of Z to avoid singularity issues
    qr_decomp <- qr(Z)
    Q <- qr.Q(qr_decomp)
    P_Z <- Q %*% t(Q)  # Projection matrix

    # Compute bias-corrected covariance
    bias_sigma <- 1/T * t(Y) %*% (I_t - P_Z) %*% Y

    # Degrees of freedom correction
    d.f. <- T / (T - Kp - 1)
    unbiased <- d.f. * bias_sigma  # Corrected covariance estimate

    return(unbiased)
}

## VAR(p) process Simulator
 var_sim <- function(AA, nu, Sigma_u, nSteps, y0) {
    K <- nrow(Sigma_u)
    Kp <- ncol(AA)
    p <- Kp/K
        
    if (p > 1) {
        C <- comp_mtrx(AA) # form the  companion matrix of the var(p) process
    } else {
        C <- AA  
    }
    y_t <- matrix(0, nrow =  nSteps, ncol=Kp) #trajectories matrix nSteps x Kp
    y_t[1, 1:Kp] <- y0 #add initial value to initiate the simulation
    noise <- mvrnorm(n = nSteps, mu = rep(0, K), Sigma = Sigma_u) #assuming that 
    #residuals follow a multivariate normal distribution    
    
    for (t in 2:nSteps) {
        y_t[t, ] <- C %*% y_t[t-1, ]
        y_t[t, 1:K] <- y_t[t, 1:K] + nu + noise[t,]
    }
    
    y_t <- zoo(y_t[,1:K], 1:nSteps)  
    return(y_t)
 }
 
 #compute the largest eigenvalues of A^TA, used to compute l2 norm 
 
lmda_max<-function(A,k){#k is the number of largest eigenvalues to compute
  s1<-svds(A,k=k, nu=0, nv=0)$d[1]
  return(s1)
}

#we then define a function to scale off-diagonal elements to ensure that 
# ||A||=target
scaled_A_matrix<- function(A,target, tol=1e-6, max_k=1000){
  D<-diag(diag(A))
  B<-A-D
  #check the l2nrom of A, that is the max eigenvalue of A^TA
  #we use hte previousy defined function lmbda_max
  f<-function(k){
    M<-D + k*B
    lmda_max(M, k=1)-target
  }
  #we evaluate at k=0
  f0<-f(0)
  if(abs(f0)<tol){
    return(D)
  }
  k_low <- 0
  k_high <- 1
  fk_high <- f(k_high)
  iter <- 0
  while ((fk_high < 0) && (k_high < max_k)) {
    k_high <- k_high * 2
    fk_high <- f(k_high)
    iter <- iter + 1
    if (iter > 200) stop("Could not bracket the root: try increasing max_k or a different initial guess.")
  }
  if (fk_high < 0) stop("Failed to find k_high that yields operator norm >= target. Increase max_k.")
  root <- uniroot(f, lower = k_low, upper = k_high, tol = tol)$root
  A_scaled <- D + root * B
  return(A_scaled)
}
```
We will now replicate figure 1 from \cite{basu2015regularized}. He provided the following values for the spectral radius $\rho(A)$ and fixed diagonal elements of the generating coefficient matrix $A$ to generate the VAR(1) process.

\begin{table}[h!]
\centering
\begin{tabular}{c|c}
$\alpha$ & $\rho(A)$ \\
\hline
0.2 & 0.2 \\
0.2 & 0.92 \\
0.2 & 0.96 \\
0.2 & 1 \\
0.2 & 1.01 \\
0.2 & 1.02 \\
0.2 & 1.03 \\
\end{tabular}
\end{table}


We will first compute a specific case, in particular for $\langle 0.2, 0.2 \rangle$ to then generalise the code for the other cases.

```{r}
K<-200
n_mc <- 200  # Number of Monte Carlo iterations

A_coef<-matrix(0L,nrow=K, ncol=K)
upper_indx<- which(row(A_coef)<col(A_coef)) 
A_coef[upper_indx]<-rnorm(length(upper_indx), mean=0, sd=0.2)
diag(A_coef)<-0.2
#check th specal norm
print(eigen(t(A_coef)%*%A_coef)$values[1]) #20 is too high, as by the simulation
#it should be around 0.2
#we then proceed to rescale
prod_coef<-t(A_coef)%*%A_coef
rho_val<-max(abs(eigen(prod_coef)$values))
a<-0.2
scalar<-a/sqrt(rho_val)
scalar
#we compute A.scaled
A_scaled<-A_coef*scalar
#check the specrral norm
round(max(abs(eigen(t(A_scaled)%*%A_scaled)$values)),2)== 0.2 # we check whether 
#the spectral norm is actually 0.2
#now we then generate the VAR(1) data and compute the response vector 
nu<-rep(0,K)
y_0<-rep(0,K)
Cov_epsilon<-diag(0.2, K)#we assume homoskedasticity
pred<-var_sim(A_scaled, nu=nu, Sigma_u=Cov_epsilon, nSteps=200, y0=y_0)

#from a stochastic proces 
#generate a sparse Kx1 beta vector to then apply the LASSO regression

beta_star<-rep(0,K)
#selectnaodmly a percentage of the coef to be different from zero
set.seed(1234)
nonzero<-K*0.3
beta_star[sample(1:K,nonzero)]<-rnorm(nonzero, mean=0, sd=0.5)

#generate gaussian inovation
epsilon_t<-rnorm(200, mean=0, sd=1)
#we generate response variables froma  linear model
y_t<- as.numeric(pred %*% beta_star) + epsilon_t
#we then compute the LASSO Regression from glmnet package
lasso_reg<-cv.glmnet(
  x=as.matrix(pred),
  y=y_t,
  type.measure="mse",
  alpha=1,
  nfolds=100,
  family="gaussian", 
  grouped=FALSE
)
beta_lasso<-coef(lasso_reg)
#now we compute the lasso error
lasso_error<-sqrt(sum((beta_lasso[-1]-beta_star)^2))
lasso_error
#we compute Monte Carlo simulation to have asymptotic results
nsteps<-100
lasso_errors<-rep(0,nsteps)
A_coef_sim<-matrix(0L,nrow=K, ncol=K)
diag(A_coef_sim)<-0.2
for(i in 1:nsteps){
upper_indx<- which(row(A_coef_sim)<col(A_coef_sim))
A_coef_sim[upper_indx]<-rnorm(length(upper_indx), mean=0, sd=0.2) 
#now we normalised the coef matrix
rho_val<-max(abs(eigen(prod_coef)$values))
a<-0.2
scalar<-a/sqrt(rho_val)
A_scaled_sim<-A_coef_sim*scalar
#simulate the data
pred_sim<-var_sim(A_scaled_sim, nu=nu, Sigma_u=Cov_epsilon, nSteps=200, y0=y_0)
#response variable
y_t_sim<- as.numeric(pred_sim %*% beta_star) + rnorm(200, mean=0, sd=1)
#LASSO regression
lasso_reg_sim<-cv.glmnet(
  x=as.matrix(pred_sim),
  y=y_t_sim,
  type.measure="mse",
  alpha=1,
  nfolds=100,
  family="gaussian", 
  grouped=FALSE
)
beta_lasso_sim<-coef(lasso_reg_sim)
#LASSO error
lasso_errors[i]<-sqrt(sum((beta_lasso_sim[-1]-beta_star)^2))
}
mu_02_02<-mean(lasso_errors)
mu_02_02
```
Now that we have define the pipeline, we will able to generalised to the other cases amd for different number of observations
```{r}
K<-200
n_mc<-100
rho_A<-0.2 # fixed, as we have triangular matrix
spctr_norm <- c(0.2, 0.92, 0.96, 1, 1.01, 1.02, 1.03)
pairs<-paste0("(",rho_A,",",spctr_norm,")")
n_values <- c(20, 40, 50, 80, 100, 150, 200, 300, 400, 500, 600) #diff level
#of timepoints
Lasso_error<-matrix(NA, nrow=length(pairs), ncol=length(n_values))
colnames(Lasso_error)<-n_values
rownames(Lasso_error)<-pairs

#fixed initial values to generate VAR(1) data#
nu <- rep(0, K)
y_0 <- rep(0, K)
Cov_epsilon <- diag(0.2, K) 

#Fixed true betas

beta_star <- rep(0, K)
nonzero <- round(K * 0.4)
beta_star[sample(1:K, nonzero)] <- runif(nonzero,2,3)

#now we start the loop 

for (i in seq_along(spctr_norm)) {
  # i is the index; norm_A is the target spectral measure for this row
  norm_A <- spctr_norm[i]
  
  for (j in seq_along(n_values)) {
    n <- n_values[j]
    lasso_iter <- rep(NA, n_mc) 
    
    # we first generate a diagonal A matrix
    # (we will re-fill the two-upper band inside the MC loop so each MC draw differs)
    
    for (m in 1:n_mc) {
      # compute the index for the two-upper triangular
      A_coef <- diag(rho_A, K, K)
      two_upper_indx<- which(col(A_coef)== row(A_coef)+2, arr.ind = TRUE)
      # we sample gamma from a normal distribution
      A_coef[ two_upper_indx] <- rnorm(nrow(two_upper_indx), mean = 0, sd = 2) #change here to have more var of VAR process

      # we now scale to ensure that spectral measure is equal to what we need
      #we scale by the spectral radius which is common 0.2
      #we need to compute the sqrt of the max eigenvalue for (A^TA)
      prod<-t(A_coef)%*% A_coef #A^TA
      rho_ATA<-max(abs(eigen(prod)$values))
      #rescale
      
      scalar<- sqrt(norm_A/rho_ATA)
      A_scaled<-A_coef * scalar
      

      # Step 2: Simulate VAR(1) data with n = n_values[j] time steps
      pred_sim <- var_sim(A_scaled, nu = nu, Sigma_u = Cov_epsilon, 
                          nSteps = n, 
                          y0 = y_0)

      # Step 3: Generate response
      y_t_sim <- as.numeric(pred_sim %*% beta_star) + rnorm(n, mean = 0,
                                                            sd = 3)
      #we asssume normality of the error term
      # Step 4: Fit LASSO
      lasso_reg_sim <- cv.glmnet(
        x = as.matrix(pred_sim),
        y = y_t_sim,
        type.measure = "deviance",
        intercept=FALSE,
        alpha = 1,
        nfolds = 5,     
        family = "gaussian",
        grouped = FALSE
      )
  beta_lasso_sim <- coef(lasso_reg_sim, s = "lambda.min") 
beta_hat <- as.numeric(beta_lasso_sim[-1])
lasso_iter <- sqrt(sum((beta_hat - beta_star)^2))  # Store error  # Store error for this MC
    } # end m loop

    # Store the average error over Monte Carlo repetitions
    Lasso_error[i, j] <- mean(lasso_iter, na.rm = TRUE) 
    
  } # end j loop
} # end i loop

Lasso_error

```

now we compute the simulation for all the cases
```{r}
K<-200
n_mc<-100
rho_A<-0.2 # fixed, as we have triangular matrix
spctr_norm <- c(0.2, 0.92, 0.96, 1, 1.01, 1.02, 1.03)
pairs<-paste0("(",rho_A,",",spctr_norm,")")
n_values <- c(20, 40, 50, 80, 100, 150, 200, 300, 400, 500, 600) #diff level
#of timepoints
Lasso_error<-matrix(NA, nrow=length(pairs), ncol=length(n_values))
colnames(Lasso_error)<-n_values
rownames(Lasso_error)<-pairs

#fixed initial values to generate VAR(1) data#
nu <- rep(0, K)
y_0 <- rep(0, K)
Cov_epsilon <- diag(0.2, K) 

#Fixed true betas

beta_star <- rep(0, K)
nonzero <- round(K * 0.4)
beta_star[sample(1:K, nonzero)] <- runif(nonzero,2,3)

#now we start the loop 

for (i in seq_along(spctr_norm)) {
  # i is the index; norm_A is the target spectral measure for this row
  norm_A <- spctr_norm[i]
    
    for (j in seq_along(n_values)) {
    n <- n_values[j]
    lasso_iter <- rep(NA, n_mc) 
    
    # we first generate a diagonal A matrix
    # (we will re-fill the two-upper band inside the MC loop so each MC draw differs)
    
    for (m in 1:n_mc) {
      # compute the index for the two-upper triangular
      A_coef <- diag(rho_A, K, K)
      two_upper_indx<- which(col(A_coef)== row(A_coef)+2, arr.ind = TRUE)
      # we sample gamma from a normal distribution
      A_coef[ two_upper_indx] <- rnorm(nrow(two_upper_indx), mean = 0, sd = 2)
      A_scaled <- scaled_A_matrix(A_coef, target = target_norm)
      
      pred_sim <- var_sim(A_scaled, nu = nu, Sigma_u = Cov_epsilon, 
                          nSteps = n, 
                          y0 = y_0)
    
      # Step 3: Generate response
      y_t_sim <- as.numeric(pred_sim %*% beta_star) + rnorm(n, mean = 0,
                                                            sd = 3)
      #we asssume normality of the error term
      # Step 4: Fit LASSO
      lasso_reg_sim <- cv.glmnet(
        x = as.matrix(pred_sim),
        y = y_t_sim,
        type.measure = "deviance",
        intercept=FALSE,
        alpha = 1,
        nfolds = 5,     
        family = "gaussian",
        grouped = FALSE
      )
     beta_lasso_sim <- coef(lasso_reg_sim, s = "lambda.1se") 
    beta_hat <- as.numeric(beta_lasso_sim[-1])
    lasso_iter <- sqrt(sum((beta_hat - beta_star)^2))  # Store error  # Store error for this MC
    } # end m loop

    # Store the average error over Monte Carlo repetitions
    Lasso_error[i, j] <- mean(lasso_iter, na.rm = TRUE) 
    
  } # end j loop
} # end i loop
  
Lasso_error
```
#plot values 

```{r}
# Convert matrix to data frame and tidy it
Lasso_error_df <- as.data.frame(Lasso_error, stringsAsFactors = FALSE)
Lasso_error_df$alpha_rho <- rownames(Lasso_error_df)

Lasso_error_long <- Lasso_error_df %>%
  pivot_longer(
    cols = -c(alpha_rho),  # Use norm_A (not rho) for ||A||
    names_to = "n_obs",
    values_to = "Lasso_Error"
  ) %>%
  mutate(n_obs = as.integer(n_obs))

# Define color and linetype palettes
color_palette <- c(
  "(0.2,0.2)"   = "black",
  "(0.2,0.92)"  = "red",
  "(0.2,0.96)"  = "blue",
  "(0.2,1)"     = "green",
  "(0.2,1.01)"  = "orange",
  "(0.2,1.02)"  = "purple",
  "(0.2,1.03)"  = "cyan"
)

line_types <- c(
  "(0.2,0.2)"   = "solid",
  "(0.2,0.92)"  = "dashed",
  "(0.2,0.96)"  = "dashed",
  "(0.2,1)"     = "dotdash",
  "(0.2,1.01)"  = "dotdash",
  "(0.2,1.02)"  = "dotdash",
  "(0.2,1.03)"  = "dotdash"
)

# Plot
g <- ggplot(
  data = Lasso_error_long,
  mapping = aes(x = n_obs, y = Lasso_Error)
) +
  geom_line(aes(color=factor(alpha_rho), linetype=factor(alpha_rho)), size=0.5) + 
  scale_color_manual(
    values = color_palette
  
  ) +
  scale_linetype_manual(
    values = line_types,  
  ) +
  labs(
    x="n",
    y=TeX("$\\|\\hat{\\beta}- \\beta^*\\|_2$"),
    color = TeX("$\\rho(A),\\,\\|A\\|_2$")  
    )+
    theme_minimal()+
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
        # make sure axes lines are visible
    axis.line = element_line(colour = "black", linewidth = 0.5),
    axis.ticks = element_line(colour = "black"),
    
    # add full black border around the plot panel
    panel.border = element_rect(colour = "black", fill = NA, linewidth = 0.8),
    
    # move legend to top-right
    legend.position = c(0.98, 0.98),
    legend.justification = c("right", "top"),
   
    legend.key.size = unit(0.5, 'cm'),
    legend.key.width = unit(0.5, 'cm'),
    legend.title = element_text(size=8),
    # make legend text a bit smaller
    legend.text = element_text(size = 6)
  )+
   guides(linetype = "none")
g
```

#Second Graphs
for the second graph we have the following situation:
$$
\text{VAR}(2)= X_j^t=2\alpha X_{j-1}^t-\alpha^2X_{j-2}^t + \xi^t
$$
We assume there is no cross-dependence, and the number of predictors is 500. 
To ensure that $\Gamma_X(0)=1$, we need to compute the covariance matrix for the residuals as follow. 
We consider the definition of Autocovariance of Stable VAR(p) process from \cite{Helmut2005}, in particular the \textit{Yule-Walker equations}:

$$
\Gamma_Y(0)=\textbf{A}\Gamma_Y(0)\textbf{A}^T + \Sigma_V, \ \text{where} \\\
A= \begin{bmatrix}
A_1 & A_2 \\
I_K & 0
\end{bmatrix}, \quad \Sigma_V = \begin{bmatrix}
\Sigma_{\epsilon} & 0 \\
0 & 0
\end{bmatrix}
$$
In our case, we assume that $\Gamma_X(0)=I_K$, so we can rearrange the previous function as follow:
$$
\Sigma_V=I_{Kp} - A \Gamma_Y(0) A^T = I_{Kp} - A A^T
$$
Unfortunately, for values closer to $\alpha$, the covariance matrix wont be positive definitive, therefore we will use numerical methods to approximate 
Sigma_u. 
$$
\begin
$$
In this scenario, the assumption $ = \||A \|<1$ is not applicabale: 
$$
\begin{align*}
  \|A\| &= \sqrt{\lambda_{max}(A^TA)}, \quad  \text{where} \quad A \
  \text{is the companion matrix}\\
  &= \begin{bmatrix}
    A_1 & A_2 \\
    I_K & 0
  \end{bmatrix} \\
  &= \begin{bmatrix}
    2\alpha I_K & \alpha^2 I_K \\
    I_K & 0
  \end{bmatrix} \\
\end{align*}
$$

```{r}
#we frist define the function too compute the VAR(2) process
#, 
#  1600
#c(0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.95)
#c(c(200, 400, 700, 1000,1300, 1500))
alpha<-0.3
n_values<-1500
K<-500
n_mc<-10
Lasso_error_ex_2<-matrix(NA,nrow=length(alpha), ncol=length(n_values))
colnames(Lasso_error_ex_2)<-as.character(n_values)
rownames(Lasso_error_ex_2)<-as.character(alpha)

#fixed initial values to generate VAR(1) dat
nu <- rep(0, K)
x_0 <- rep(0, K*2)

#we generate sparse coef matrix for the stochastic process
set.seed(2025)  
beta_star_fixed <- numeric(2 * K)
  nonzero_first_lag  <- round(K * 0.4)
  nonzero_second_lag <- round(K * 0.4)
  beta_star_fixed[sample(1:K, nonzero_first_lag)] <- runif(nonzero_first_lag, 2, 3)
  beta_star_fixed[sample((K + 1):(2 * K), nonzero_second_lag)] <- runif(nonzero_second_lag, 2, 3)

#we define the true coef. matrix for the stochastic process
    for(k in seq_along(alpha)){
      #we first compute the Sigma_V as previously described
      alpha_i<-alpha[k]
      
      S<-(1+alpha_i)^2/(1-alpha_i^2)^3
      sigma_2<- 1/S
      #now we compute Sigma_V
      Sigma_V<-diag(sigma_2, nrow=K, ncol=K)
      
      #to compute predictors, we will compute VAR(2) as a VAR(1), as described 
      #in Hamilton(1995)
      
      A1 <- diag(2* alpha,K,K) #no cross-sectional dependence
      A2<-diag(-(alpha^2),K,K)
      AA <- cbind(A1, A2)
      
      #compute companion matrix 
      AA_comp<-comp_mtrx(AA,2)
      
    for(j in 1:length(n_values)){
      n<- n_values[j]
    lasso_iter <- rep(NA, n_mc)
    for(i in 1:n_mc){  
    X_t <- matrix(0, nrow = n, ncol = 2 * K)  # trajectories matrix nSteps x Kp
      X_t[1, ] <- x_0  # add initial value to initiate the simulation

      # Generate noise
      noise <- mvrnorm(n = n, mu = rep(0, K), Sigma = Sigma_V)

      # Simulation loop
      for (t in 2:n) {
      X_t[t, ] <- AA_comp %*% X_t[t-1, ]
      X_t[t, 1:K] <- X_t[t, 1:K] + nu + noise[t, ]
      }
      
      #we generate beta_star
      beta_star <- matrix(0, 2 * K)
      nonzero_first_lag <- round(K * 0.4)
      nonzero_second_lag <- round(K * 0.4)
      beta_star[sample(1:K, nonzero_first_lag)] <- runif(nonzero_first_lag, 2, 3)
      beta_star[sample((K + 1):(2 * K), nonzero_second_lag)] <- runif(nonzero_second_lag, 2, 3)

      beta_star <- as.numeric(beta_star)
  
      y_t_sim <- as.numeric(X_t %*% beta_star) + rnorm(n, mean = 0, sd = 3)
      #we asssume normality of the error term
      # Step 4: Fit LASSO
      lasso_reg_sim <- cv.glmnet(
        x = as.matrix(X_t),
        y = as.matrix(y_t_sim),
        intercept=FALSE,
        alpha = 1,
        nfolds = 5, 
        grouped = FALSE
      )
    beta_lasso_sim <- coef(lasso_reg_sim, s = "lambda.min") 
    beta_hat <- as.numeric(beta_lasso_sim[-1])
lasso_iter[i] <- sqrt(sum((beta_hat - beta_star)^2))
    
    }
    Lasso_error_ex_2[k, j] <- mean(lasso_iter, na.rm = TRUE) 
   }
  }
Lasso_error_ex_2

```


