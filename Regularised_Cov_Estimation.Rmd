---
title: "Regularised Cov. Estimation"
author: "Jacopo Lussetti"
date: "`r Sys.Date()`"
output:
  pdf_document:
    citation_package: natbib
  html_document:
    citation_package: natbib
header-includes:
  - \usepackage{amsmath}
  - \usepackage{amssymb}
  - \usepackage{natbib}
bibliography: reference.bib
---
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(MASS)
library(zoo)
library(stats)
library(vars)
library(glmnet)
library(ggplot2)
library(tidyr)
library(dplyr)
library(latex2exp) #write expression with latex in ggplot
library(RSpectra)
library(gridExtra)
library(Matrix)
```

This report is aiming at replicating the methodology by @basu2015regularized
on estimating covariance matrices and their precision matrices for high-dimensional data.

Previous studies on the covariance estimator(@chen2013covandprecision) 
found out that the functional dependence measure scale
with the spectral radius $\rho(A)$. 
We define functional dependence measure scale quantify the magnitude of time 
dependency between current and past observations. More formally, 

\begin{equation}
\theta_{i,w,j}=\|Z_{ji}-Z'_{ji} \|_w= \left(\mathbb{E} \lvert Z_{ji}-Z'_{ji} \lvert^w \right)^{\frac{1}{w}}
\end{equation}

where $Z_{ji}$ are stationary process, and $Z'_{ji}=g(\mathcal{F'_i})$ with 
the filtration being defined as $\mathcal{F}^i(\ldots e_{-1}, e'_0, e_1, \ldots e_i)$,
for $e'_0$ is an independent copy of the original innovation. 
Additionally, we also consider the Short-Range Dependence:

\begin{equation}
  \Theta_{mw}=\max_{1\leq j \leq p}\sum_{\ell=m}^{\infty}\theta_{i,w,j}<\infty
\end{equation}

In example 2.2 on stationary linear processes, he considers $\textbf{z}_i=\sum_{m=0}^{\infty}A_m e_{i-m}$
with $\Sigma_e= \mathbb{E}(\textbf{e}_i\textbf{e}_i^T),  \Sigma_{z_i}=\sum_{m=0}^{\infty}A_m\Sigma_eA_m^T$.
We assume that $e_{ij}$ i iid with mean 0, variance 1, $A_i=(a_{i,jk})_{1 \leq j, k \leq q} \quad \text{s.t.} \max_{j \leq p }\sum_{k=1}^p a_{i,jk}^2= \mathcal{O}(i^{-2 -2 \gamma})$. Then by the Rosenthal's theory:
$$
\mathbb{E}\left[\left|\sum_{i=1}^n X_i\right|^p\right] \leq C_p \left( \sum_{i=1}^n \mathbb{E}[|X_i|^p] + \left(\sum_{i=1}^n \mathbb{E}[X_i^2]\right)^{p/2} \right)
$$


In this setting, for the $j$th component we have 
$Z_{ji}=\sum_{m=0}^{\infty}\sum_{k=1}^p a_{m,jk} e_{k,i-m}$, 
and the coupled version differs only in the innovation at time $0$:
$$
Z_{ji}-Z'_{ji}=\sum_{k=1}^p a_{i,jk}(e_{k0}-e'_{k0}).
$$
Applying Rosenthalâ€™s inequality to the independent summands 
$X_k=a_{i,jk}(e_{k0}-e'_{k0})$ gives
$$
\mathbb{E}\!\left|\sum_{k}X_k\right|^{w}
\le C_w\!\left(
\sum_{k}|a_{i,jk}|^{w}+\Big(\sum_{k}a_{i,jk}^2\Big)^{w/2}
\right),
$$
and therefore
$$
\theta_{i,w,j}=\|Z_{ji}-Z'_{ji}\|_w
\le C_w\Big(\sum_{k=1}^p a_{i,jk}^2\Big)^{1/2}.
$$
Under the assumption 
$\max_{j\le p}\sum_{k=1}^p a_{i,jk}^2=\mathcal{O}(i^{-2-2\gamma})$,
we obtain $\theta_{i,w,j}=\mathcal{O}(i^{-1-\gamma})$ and hence
$$
\Theta_{mw}
=\max_{1\le j\le p}\sum_{\ell=m}^{\infty}\theta_{\ell,w,j}
=\mathcal{O}(m^{-\gamma}),
$$
A special case of equation 1 is VAR(1) process $\textbf{z}_i=A \textbf{z}_{t-1}+\textbf{e}_i$, where A is a real matrix with spectral norm $\rho(A)<1$, and the functional dependence measure $\theta_{i,2q,j}=\mathcal{O}(\rho(A)^i)$

# Exercise 1 

##Data Generation
To simulate the stochastic regression model, we will generate data from our code
that allows to randomly generate an uppper triangle coefficient matrix. Then we
will simulate the data with a VAR(1) model. Then we will proceed to use a 
Gaussian VAR(1) model \\ 

$$
  X^t=AX^{t-1} + \epsilon^t, \quad \epsilon^t \overset{\text{i.i.d.}}{\sim}\mathcal{N}(0, \sigma^2 I_{Kp}), \quad \text{and}
\quad \text{diag}(A) =0.2
$$
  We first define a series of equations that we will use throughout the simulation.
To ensure that l2-norm of the coefficient matrix is equal to a target
value, we apply the following procedure.
```{r}
#we define first of all the various functions
##Simulation upper triangular matrix
### To ensure that process is stable, we need to check the abs of eigenvalue
stab_test <- function(kp, A, tol = 1e-8)
{
  if (!is.matrix(A) || nrow(A) != ncol(A)) {
    stop("The matrix is not square")
  }
  eig <- eigen(A, only.values = TRUE)$values  # computing the eigenvalues
  
  for (i in 1:length(eig)) {     
    if (Mod(eig[i]) >= 1 - tol) { 
      return(FALSE)               
    }
  }
  return(TRUE)
}


#function to compute companion matrix
comp_mtrx <- function(AA,p ){ #AA is the cbind of covariate matrices
  K<-nrow(AA)
  Kp<-K*p
  C<-matrix(0, nrow=Kp, ncol=Kp)
  C[1:K,1:Kp]<-AA
  C[(K+1):Kp,1:(K*(p-1))]<- diag(1,nrow=K, ncol=K )
  return(C)
}

#
est_autocov <- function(y_t, Y, Z, T, p=1){
  K <- ncol(y_t)
  Kp <- K * p
  I_t <- diag(T)
  
  # QR decomposition of Z to avoid singularity issues
  qr_decomp <- qr(Z)
  Q <- qr.Q(qr_decomp)
  P_Z <- Q %*% t(Q)  # Projection matrix
  
  # Compute bias-corrected covariance
  bias_sigma <- 1/T * t(Y) %*% (I_t - P_Z) %*% Y
  
  # Degrees of freedom correction
  d.f. <- T / (T - Kp - 1)
  unbiased <- d.f. * bias_sigma  # Corrected covariance estimate
  
  return(unbiased)
}

## VAR(p) process Simulator
var_sim <- function(AA, nu, Sigma_u, nSteps, y0) {
  K <- nrow(Sigma_u)
  Kp <- ncol(AA)
  p <- Kp/K
  
  if (p > 1) {
    C <- comp_mtrx(AA) # form the  companion matrix of the var(p) process
  } else {
    C <- AA  
  }
  y_t <- matrix(0, nrow =  nSteps, ncol=Kp) #trajectories matrix nSteps x Kp
  y_t[1, 1:Kp] <- y0 #add initial value to initiate the simulation
  noise <- mvrnorm(n = nSteps, mu = rep(0, K), Sigma = Sigma_u) #assuming that 
  #residuals follow a multivariate normal distribution    
  
  for (t in 2:nSteps) {
    y_t[t, ] <- C %*% y_t[t-1, ]
    y_t[t, 1:K] <- y_t[t, 1:K] + nu + noise[t,]
  }
  
  y_t <- zoo(y_t[,1:K], 1:nSteps)  
  return(y_t)
}

#compute the largest eigenvalues of A^TA, used to compute l2 norm 

lmda_max<-function(A,k){#k is the number of largest eigenvalues to compute
  s1<-svds(A,k=k, nu=0, nv=0)$d[1]
  return(s1)
}

#we then define a function to scale off-diagonal elements to ensure that 
# ||A||=target
scaled_A_matrix<- function(A,target, tol=1e-6, max_k=1000){
  D<-diag(diag(A))
  B<-A-D
  #check the l2nrom of A, that is the max eigenvalue of A^TA
  #we use hte previousy defined function lmbda_max
  f<-function(k){
    M<-D + k*B
    lmda_max(M, k=1)-target
  }
  #we evaluate at k=0
  f0<-f(0)
  if(abs(f0)<tol){
    return(D)
  }
  k_low <- 0
  k_high <- 1
  fk_high <- f(k_high)
  iter <- 0
  while ((fk_high < 0) && (k_high < max_k)) {
    k_high <- k_high * 2
    fk_high <- f(k_high)
    iter <- iter + 1
    if (iter > 200) stop("Could not bracket the root: try increasing max_k or a different initial guess.")
  }
  if (fk_high < 0) stop("Failed to find k_high that yields operator norm >= target. Increase max_k.")
  root <- uniroot(f, lower = k_low, upper = k_high, tol = tol)$root
  A_scaled <- D + root * B
  return(A_scaled)
}
```

We will now replicate figure 1 from @basu2015regularized. He provided the following values for the spectral radius $\rho(A)$ and fixed diagonal elements of the generating coefficient matrix $A$ to generate the VAR(1) process.

\begin{table}[h!]
\centering
\begin{tabular}{c|c}
$\alpha$ & $\rho(A)$ \\
\hline
0.2 & 0.2 \\
0.2 & 0.92 \\
0.2 & 0.96 \\
0.2 & 1 \\
0.2 & 1.01 \\
0.2 & 1.02 \\
0.2 & 1.03 \\
\end{tabular}
\end{table}


We will first compute a specific case, in particular for $\langle 0.2, 0.2 \rangle$
to then generalise the code for the other cases.
  
```{r}
K<-200
n_mc <- 200  # Number of Monte Carlo iterations

A_coef<-matrix(0L,nrow=K, ncol=K)
upper_indx<- which(row(A_coef)<col(A_coef)) 
A_coef[upper_indx]<-rnorm(length(upper_indx), mean=0, sd=0.2)
diag(A_coef)<-0.2
#check th specal norm
print(eigen(t(A_coef)%*%A_coef)$values[1]) #20 is too high, as by the simulation
#it should be around 0.2
#we then proceed to rescale
prod_coef<-t(A_coef)%*%A_coef
rho_val<-max(abs(eigen(prod_coef)$values))
a<-0.2
scalar<-a/sqrt(rho_val)
scalar
#we compute A.scaled
A_scaled<-A_coef*scalar
#check the specrral norm
round(max(abs(eigen(t(A_scaled)%*%A_scaled)$values)),2)== 0.2 # we check whether 
#the spectral norm is actually 0.2
#now we then generate the VAR(1) data and compute the response vector 
nu<-rep(0,K)
y_0<-rep(0,K)
Cov_epsilon<-diag(0.2, K)#we assume homoskedasticity
pred<-var_sim(A_scaled, nu=nu, Sigma_u=Cov_epsilon, nSteps=200, y0=y_0)

#from a stochastic proces 
#generate a sparse Kx1 beta vector to then apply the LASSO regression

beta_star<-rep(0,K)
#selectnaodmly a percentage of the coef to be different from zero
set.seed(1234)
nonzero<-K*0.3
beta_star[sample(1:K,nonzero)]<-rnorm(nonzero, mean=0, sd=0.5)

#generate gaussian inovation
epsilon_t<-rnorm(200, mean=0, sd=1)
#we generate response variables froma  linear model
y_t<- as.numeric(pred %*% beta_star) + epsilon_t
#we then compute the LASSO Regression from glmnet package
lasso_reg<-cv.glmnet(
  x=as.matrix(pred),
  y=y_t,
  type.measure="mse",
  alpha=1,
  nfolds=100,
  family="gaussian", 
  grouped=FALSE
)
beta_lasso<-coef(lasso_reg)
#now we compute the lasso error
lasso_error<-sqrt(sum((beta_lasso[-1]-beta_star)^2))
lasso_error
#we compute Monte Carlo simulation to have asymptotic results
nsteps<-100
lasso_errors<-rep(0,nsteps)
A_coef_sim<-matrix(0L,nrow=K, ncol=K)
diag(A_coef_sim)<-0.2
for(i in 1:nsteps){
  upper_indx<- which(row(A_coef_sim)<col(A_coef_sim))
  A_coef_sim[upper_indx]<-rnorm(length(upper_indx), mean=0, sd=0.2) 
  #now we normalised the coef matrix
  rho_val<-max(abs(eigen(prod_coef)$values))
  a<-0.2
  scalar<-a/sqrt(rho_val)
  A_scaled_sim<-A_coef_sim*scalar
  #simulate the data
  pred_sim<-var_sim(A_scaled_sim, nu=nu, Sigma_u=Cov_epsilon, nSteps=200, y0=y_0)
  #response variable
  y_t_sim<- as.numeric(pred_sim %*% beta_star) + rnorm(200, mean=0, sd=1)
  #LASSO regression
  lasso_reg_sim<-cv.glmnet(
    x=as.matrix(pred_sim),
    y=y_t_sim,
    type.measure="mse",
    alpha=1,
    nfolds=100,
    family="gaussian", 
    grouped=FALSE
  )
  beta_lasso_sim<-coef(lasso_reg_sim)
  #LASSO error
  lasso_errors[i]<-sqrt(sum((beta_lasso_sim[-1]-beta_star)^2))
}
mu_02_02<-mean(lasso_errors)
mu_02_02
```
Now that we have define the pipeline, we will able to generalised to the other 
cases amd for different number of observations
```{r}
K<-200
n_mc<-100
rho_A<-0.2 # fixed, as we have triangular matrix
spctr_norm <- c(0.2, 0.92, 0.96, 1, 1.01, 1.02, 1.03)
pairs<-paste0("(",rho_A,",",spctr_norm,")")
n_values <- c(20, 40, 50, 80, 100, 150, 200, 300, 400, 500, 600) #diff level
#of timepoints
Lasso_error<-matrix(NA, nrow=length(pairs), ncol=length(n_values))
colnames(Lasso_error)<-n_values
rownames(Lasso_error)<-pairs

#fixed initial values to generate VAR(1) data#
nu <- rep(0, K)
y_0 <- rep(0, K)
Cov_epsilon <- diag(0.2, K) 

#Fixed true betas

beta_star <- rep(0, K)
nonzero <- round(K * 0.4)
beta_star[sample(1:K, nonzero)] <- runif(nonzero,2,3)

#now we start the loop 

for (i in seq_along(spctr_norm)) {
  # i is the index; norm_A is the target spectral measure for this row
  norm_A <- spctr_norm[i]
  
  for (j in seq_along(n_values)) {
    n <- n_values[j]
    lasso_iter <- rep(NA, n_mc) 
    
    # we first generate a diagonal A matrix
    # (we will re-fill the two-upper band inside the MC loop so each MC draw differs)
    
    for (m in 1:n_mc) {
      # compute the index for the two-upper triangular
      A_coef <- diag(rho_A, K, K)
      two_upper_indx<- which(col(A_coef)== row(A_coef)+2, arr.ind = TRUE)
      # we sample gamma from a normal distribution
      A_coef[ two_upper_indx] <- rnorm(nrow(two_upper_indx), mean = 0, sd = 2) 
      #change here to have more var of VAR process
      
      # we now scale to ensure that spectral measure is equal to what we need
      #we scale by the spectral radius which is common 0.2
      #we need to compute the sqrt of the max eigenvalue for (A^TA)
      prod<-t(A_coef)%*% A_coef #A^TA
      rho_ATA<-max(abs(eigen(prod)$values))
      #rescale
      
      scalar<- sqrt(norm_A/rho_ATA)
      A_scaled<-A_coef * scalar
      
      
      # Step 2: Simulate VAR(1) data with n = n_values[j] time steps
      pred_sim <- var_sim(A_scaled, nu = nu, Sigma_u = Cov_epsilon, 
                          nSteps = n, 
                          y0 = y_0)
      
      # Step 3: Generate response
      y_t_sim <- as.numeric(pred_sim %*% beta_star) + rnorm(n, mean = 0,
                                                            sd = 3)
      #we asssume normality of the error term
      # Step 4: Fit LASSO
      lasso_reg_sim <- cv.glmnet(
        x = as.matrix(pred_sim),
        y = y_t_sim,
        type.measure = "deviance",
        intercept=FALSE,
        alpha = 1,
        nfolds = 5,     
        family = "gaussian",
        grouped = FALSE
      )
      beta_lasso_sim <- coef(lasso_reg_sim, s = "lambda.min") 
      beta_hat <- as.numeric(beta_lasso_sim[-1])
      lasso_iter <- sqrt(sum((beta_hat - beta_star)^2)) 
    } # end m loop
    
    # Store the average error over Monte Carlo repetitions
    Lasso_error[i, j] <- mean(lasso_iter, na.rm = TRUE) 
    
  } # end j loop
} # end i loop

Lasso_error

```

now we compute the simulation for all the cases
```{r}
K<-200
n_mc<-100
rho_A<-0.2 # fixed, as we have triangular matrix
spctr_norm <- c(0.2, 0.92, 0.96, 1, 1.01, 1.02, 1.03)
pairs<-paste0("(",rho_A,",",spctr_norm,")")
n_values <- c(20, 40, 50, 80, 100, 150, 200, 300, 400, 500, 600) #diff level
#of timepoints
Lasso_error<-matrix(NA, nrow=length(pairs), ncol=length(n_values))
colnames(Lasso_error)<-n_values
rownames(Lasso_error)<-pairs

#fixed initial values to generate VAR(1) data#
nu <- rep(0, K)
y_0 <- rep(0, K)
Cov_epsilon <- diag(0.2, K) 

#Fixed true betas

beta_star <- rep(0, K)
nonzero <- round(K * 0.4)
beta_star[sample(1:K, nonzero)] <- runif(nonzero,2,3)

#now we start the loop 

for (i in seq_along(spctr_norm)) {
  # i is the index; norm_A is the target spectral measure for this row
  norm_A <- spctr_norm[i]
  
  for (j in seq_along(n_values)) {
    n <- n_values[j]
    lasso_iter <- rep(NA, n_mc) 
    
    
    
    for (m in 1:n_mc) {
      # compute the index for the two-upper triangular
      A_coef <- diag(rho_A, K, K)
      two_upper_indx<- which(col(A_coef)== row(A_coef)+2, arr.ind = TRUE)
      # we sample gamma from a normal distribution
      A_coef[ two_upper_indx] <- rnorm(nrow(two_upper_indx), mean = 0, sd = 2)
      A_scaled <- scaled_A_matrix(A_coef, target = norm_A)
      
      pred_sim <- var_sim(A_scaled, nu = nu, Sigma_u = Cov_epsilon, 
                          nSteps = n, 
                          y0 = y_0)
      
      # Step 3: Generate response
      y_t_sim <- as.numeric(pred_sim %*% beta_star) + rnorm(n, mean = 0,
                                                            sd = 3)
      #we asssume normality of the error term
      # Step 4: Fit LASSO
      lasso_reg_sim <- cv.glmnet(
        x = as.matrix(pred_sim),
        y = y_t_sim,
        type.measure = "deviance",
        intercept=FALSE,
        alpha = 1,
        nfolds = 5,     
        family = "gaussian",
        grouped = FALSE
      )
      beta_lasso_sim <- coef(lasso_reg_sim, s = "lambda.1se") 
      beta_hat <- as.numeric(beta_lasso_sim[-1])
      lasso_iter <- sqrt(sum((beta_hat - beta_star)^2)) 
    } # end m loop
    
    # Store the average error over Monte Carlo repetitions
    Lasso_error[i, j] <- mean(lasso_iter, na.rm = TRUE) 
    
  } # end j loop
} # end i loop

Lasso_error
```
## Visualisation 

```{r,warning=FALSE}
# Convert matrix to data frame and tidy it
Lasso_error_df <- as.data.frame(Lasso_error, stringsAsFactors = FALSE)
Lasso_error_df$alpha_rho <- rownames(Lasso_error_df)

Lasso_error_long <- Lasso_error_df %>%
  pivot_longer(
    cols = -c(alpha_rho),  # Use norm_A (not rho) for ||A||
    names_to = "n_obs",
    values_to = "Lasso_Error"
  ) %>%
  mutate(n_obs = as.integer(n_obs))

# Define color and linetype palettes
color_palette <- c(
  "(0.2,0.2)"   = "black",
  "(0.2,0.92)"  = "red",
  "(0.2,0.96)"  = "grey",
  "(0.2,1)"     = "purple",
  "(0.2,1.01)"  = "cyan",
  "(0.2,1.02)"  = "#FA008F",
  "(0.2,1.03)"  = "yellow"
)

line_types <- c(
  "(0.2,0.2)"   = "solid",
  "(0.2,0.92)"  = "dashed",
  "(0.2,0.96)"  = "dashed",
  "(0.2,1)"     = "dotdash",
  "(0.2,1.01)"  = "dotdash",
  "(0.2,1.02)"  = "dotdash",
  "(0.2,1.03)"  = "dotdash"
)

# Plot
g <- ggplot(
  data = Lasso_error_long,
  mapping = aes(x = n_obs, y = Lasso_Error)
) +
  geom_line(aes(color=factor(alpha_rho), linetype=factor(alpha_rho)), size=0.5) + 
  scale_color_manual(
    values = color_palette
    
  ) +
  scale_linetype_manual(
    values = line_types,  
  ) +
  labs(
    x="n",
    y=TeX("$\\|\\hat{\\beta}- \\beta^*\\|_2$"),
    color = TeX("$\\rho(A),\\,\\|A\\|_2$")  
  )+
  expand_limits(x=700)+
  theme_minimal()+
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    # make sure axes lines are visible
    axis.line = element_line(colour = "black", linewidth = 0.5),
    axis.ticks = element_line(colour = "black"),
    
    # add full black border around the plot panel
    panel.border = element_rect(colour = "black", fill = NA, linewidth = 0.8),
    
    # move legend to top-right
    legend.position = c(0.98, 0.98),
    legend.justification = c("right", "top"),
    
    legend.key.size = unit(0.5, 'cm'),
    legend.key.width = unit(0.5, 'cm'),
    legend.title = element_text(size=8),
    # make legend text a bit smaller
    legend.text = element_text(size = 6)
  )+
  guides(linetype = "none")

```

#Exercise 2

for the second graph we have the following situation:
$$
  \text{VAR}(2)= X_j^t=2\alpha X_{j-1}^t-\alpha^2X_{j-2}^t + \xi^t
$$
  
  We assume there is no cross-dependence, and the data is centered. and the number of predictors is 500. 
To ensure that $\Gamma_X(0)=1$, we need to compute the covariance matrix for the residuals as follow. 
We consider the definition of Autocovariance of Stable VAR(p) process from \cite{Helmut2005}, in particular the \textit{Yule-Walker equations}:
  
  $$
  \Gamma_Y(0)=\textbf{A}\Gamma_Y(0)\textbf{A}^T + \Sigma_V, \ \text{where} \\\
A= \begin{bmatrix}
A_1 & A_2 \\
I_K & 0
\end{bmatrix}, \quad \Sigma_V = \begin{bmatrix}
\Sigma_{\epsilon} & 0 \\
0 & 0
\end{bmatrix}
$$
  In our case, we assume that $\Gamma_X(0)=I_K$, so we can rearrange the previous function as follow:
  $$
  \Sigma_V=I_{Kp} - A \Gamma_Y(0) A^T = I_{Kp} - A A^T
$$
  Unfortunately, for values closer to $\alpha$, the covariance matrix wont be positive definitive, but we could rely on the fact that there is no cross-dependence, implying that the autocovariance is determined by the scaled residual variance. 
We considered the VAR(1) representation of the VAR(2) process:
$$
  X_t=\alpha_iX_{t-1 } +\xi_t
$$
\begin{align*}
\xi_t &=X_t-2\alpha X_{t-1}+\alpha^2X_{t-2}\\
&=\underbrace{(1-2\alpha +\alpha^2L^2)}_{(1-\alpha L)^2}X_t \ && \text{where} \ L=\text{Lag Operator}\\
X_t& =(1-\alpha L)^{-2}X_t\\
(1-\alpha L)^{-2}&= \left[\sum_{j=0}^{\infty}(\alpha L)^j\right]^2\\
&=\sum_{j=0}^{\infty}(j+1)(\alpha L)^j \\
X_t&=\underbrace{\sum_{j=0}^{\infty}(j+1)(\alpha )^j \xi_{t-j}}_{\text{Wold Representation}}\\
\Gamma_X(0)=\text{Var}(X_t)&=\sigma_{\xi} \underbrace{\sum_{j=0}^{\infty}(j+1)^2(\alpha)^j}_{(m^2-2m+1)r^m} 
&&\sum_{j=0}^{\infty}r^m=\frac{1}{1-r}        \\
\sum_{j=0}^{\infty}mr^{m-1}&= \frac{1}{(1-r)^2} \rightarrow mr^m=\frac{r}{(1-r)^2} 
&& \sum_{j=0}^{\infty}m^2r^m=\frac{r+r^2}{(1-r)^3}    \\
\underbrace{\Gamma_X(0)}_{\mathbb{I}}&= \sigma_{\xi}\frac{(1+\alpha^2)}{(1-\alpha^2)^3}\\
\sigma_{\xi}&=\frac{(1-\alpha^2)^3}{(1+\alpha^2)}\mathbb{I}&& \\
\end{align*}

```{r}
#we start listing all constant and paramters 
alpha <- c(0.1, 0.3, 0.5,0.6, 0.7, 0.8, 0.9, 0.95)
n_values <- c(200, 400, 700, 1000, 1300, 1500)
K <- 500
n_mc <- 50

nu <- rep(0, K)
x_0 <- rep(0, K * 2)

# we set sparsity in the true beta for both 
nonzero_first_lag  <- round(K * 0.4)
nonzero_second_lag <- round(K * 0.4)
#store lasso error
Lasso_error_ex_2<-matrix(NA,nrow=length(alpha), ncol=length(n_values))

for(a_idx in seq_along(alpha)) {
  alpha_i <- alpha[a_idx]
  
  S <- (1 + alpha_i)^2 / (1 - alpha_i^2)^3
  sigma_2 <- 1 / S
  Sigma_V <- diag(sigma_2, nrow = K, ncol = K)
  
  A1 <- diag(2 * alpha_i, K, K)
  A2 <- diag(-(alpha_i^2), K, K)
  AA <- cbind(A1, A2)
  AA_comp <- comp_mtrx(AA, 2)
  
  if(max(abs(eigen(AA_comp, only.values = TRUE)$values)) >= 1) {
    warning("Alpha ", alpha_i, ": companion matrix spectral radius >= 1 (process may be unstable)")
  }
  
  for(n_idx in seq_along(n_values)) {
    #check how long it takes for each loop 
    start_time <- Sys.time()
    n <- n_values[n_idx]
    
    errs <- numeric(n_mc)
    nnz  <- integer(n_mc)
    tol_level_vec <- numeric(n_mc)
    
    
    #we generate the true parameters for the stochastic regression
    set.seed(2025)                         
    beta_star <- numeric(2 * K)
    beta_star[sample(1:K, nonzero_first_lag)] <- runif(nonzero_first_lag, 2, 3)
    beta_star[sample((K + 1):(2 * K), nonzero_second_lag)] <- runif(nonzero_second_lag, 2, 3)
    
    for(mc in 1:n_mc) {
      # simulate X_t for this MC iteration
      X_t <- matrix(0, nrow = n, ncol = 2 * K)
      X_t[1, ] <- x_0
      noise <- mvrnorm(n = n, mu = rep(0, K), Sigma = Sigma_V)
      #VAR(1) process
      for(t in 2:n) {
        X_t[t, ] <- AA_comp %*% X_t[t - 1, ]
        X_t[t, 1:K] <- X_t[t, 1:K] + nu + noise[t, ]
      }
      
      y_t_sim <- as.numeric(X_t %*% beta_star) + rnorm(n, mean = 0, sd = 3)
      
      lasso_reg_sim <- cv.glmnet(
        x = as.matrix(X_t),
        y = as.numeric(y_t_sim),
        intercept = FALSE, #we remove the intercept as we have 
        #nu=0 in the original model.
        alpha = 1, #this ensure we applied a LASSO regerssion
        nfolds = 10,
        grouped = FALSE
      )
      
      coef_hat <- as.numeric(coef(lasso_reg_sim, s = "lambda.min")[-1])
      errs[mc] <- sqrt(sum((coef_hat - beta_star)^2))
      
      
    } # end MC loop
    end_time <- Sys.time()
    elapsed <- round(as.numeric(difftime(end_time, start_time, units = "mins")), 2)
    valid_errs <- errs[!is.na(errs)]
    Lasso_error_ex_2[ a_idx,n_idx]<-mean(valid_errs, na.rm = TRUE)
    cat("alpha=", alpha_i, " n=", n, " MC valid=", length(valid_errs),
        " median_err=", round(median(valid_errs, na.rm = TRUE), 4),
        " mean_err=", round(mean(valid_errs, na.rm = TRUE), 4),
        " sd=", round(sd(valid_errs, na.rm = TRUE), 4), "\n")
    
    cat("  quantiles (0.1,0.25,0.5,0.75,0.9): ",
        paste(round(quantile(valid_errs, probs = c(.1, .25, .5, .75, .9),
                             na.rm = TRUE), 3), collapse = ", "), "\n")
    
    cat("Execution time for each (alpha,n) combination", elapsed)
    
    
  } # end n_values loop
  
} # end alpha loop

```

```{r, warning=FALSE}
#Convert names from the graph
colnames(Lasso_error_ex_2)<-as.character(n_values)
rownames(Lasso_error_ex_2)<-as.character(alpha)
# Convert matrix to data frame and tidy it
Lasso_error_df2 <- as.data.frame(Lasso_error_ex_2, stringsAsFactors = FALSE)
Lasso_error_df2$alpha <- rownames(Lasso_error_df2)

Lasso_error_long <- Lasso_error_df2 %>%
  pivot_longer(
    cols = -c(alpha),  
    names_to = "n_obs",
    values_to = "Lasso_Error"
  ) %>%
  mutate(n_obs = as.integer(n_obs))

# Define color and linetype palettes
color_palette <- c(
  "0.1"   = "black",
  "0.3"  = "red",
  "0.5"  = "grey",
  "0.6"     = "purple",
  "0.7"  = "cyan",
  "0.8"  = "#FA008F",
  "0.90"  = "yellow",
  "0.95"  = "blue"
)

line_types <- c(
  "0.1"   = "solid",
  "0.3"  = "dashed",
  "0.5"  = "dashed",
  "0.6"     = "dotdash",
  "0.7"  = "dotdash",
  "0.8"  = "dotdash",
  "0.90"  = "dotdash",
  "0.95"  ="solid"
)

# Plot
p <- ggplot(
  data = Lasso_error_long,
  mapping = aes(x = n_obs, y = Lasso_Error)
) +
  geom_line(aes(color=factor(alpha), linetype=factor(alpha)), size=0.5) + 
  scale_color_manual(
    values = color_palette
    
  ) +
  scale_linetype_manual(
    values = line_types,  
  ) + 
  scale_x_continuous(breaks=seq(0,1600, by=400))+
  scale_y_continuous(breaks = seq(0,60, by=10))+
  expand_limits(x=1700)+
  labs(
    x="n",
    y=TeX("$\\|\\hat{\\beta}- \\beta^*\\|_2$"),
    color = TeX("$\\alpha$")  
  )+
  theme_minimal()+
  theme(
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    # make sure axes lines are visible
    axis.line = element_line(colour = "black", linewidth = 0.5),
    axis.ticks = element_line(colour = "black"),
    
    # add full black border around the plot panel
    panel.border = element_rect(colour = "black", fill = NA, linewidth = 0.8),
    
    # move legend to top-right
    legend.position = c(0.98, 0.98),
    legend.justification = c("right", "top"),
    
    legend.key.size = unit(0.5, 'cm'),
    legend.key.width = unit(0.5, 'cm'),
    legend.title = element_text(size=8),
    
    legend.text = element_text(size = 6),
    
    
  )+
  guides(linetype = "none")

```

```{r}
#combine graphs 
gp<-grid.arrange(g,p,nrow=2)
gp
```
On the top figure,  there exhist a cross-sectional dependence defined by the 
VAR coefficient vectors. We saw at the beginning of the report that the 
functional dependence scale is of the same order as $\rho(A)$, implying that 
$\theta \leq c \cdot \rho(A)$ with sufficiently large n. In the picture however
we see that even though we fixed $\rho(A)$, by increasing l2 norm of A 
the estimation error decreases more slowly with increasing sample size n. 
This indicates that the estimation error is sensitive to the magnitude of the 
coefficients in the matrix A. Potentially A can slightly exceed 1 as well.

On the second example we have a situation that in some cases the assumption
$\| A\|<1$ might not hold. Assmuing that there is no cross-dependence, the graph
shows different convergence speeds depending on $\alpha$, even though all processes are
stable.We notice that for certain level of alpha, we have that with large sample 
size, the effect of dependence is significantly reduced. 

